\chapter{DELF}\label{delf_chapter}
Das DELF-Verfahren \cite{delf} von Noh, \mbox{Araujo} et al. bildet die Basis für die Experimente, die in dieser Arbeit durchgeführt werden.
In dem zu DELF veröffentlichten Artikel werden einige Verfahrensschritte nicht, oder nur oberflächlich beschrieben, was als Grundlage für eine detailgetreue Neuimplementierung nicht ausreicht. Die Autoren stellen ebenfalls den Quellcode einer Implementierung des DELF-Verfahrens zur Verfügung\footnote{\url{https://github.com/tensorflow/models/tree/master/research/delf}, zuletzt besucht 16.06.20}. Dieser weist jedoch einige konzeptionelle Unterschiede zu dem im Artikel beschriebenen Vorgehen auf. Für den Experimentalteil dieser Arbeit wurde DELF neu implementiert\footnote{\url{https://gitlab.hrz.tu-chemnitz.de/s5407552--tu-dresden.de/hist4delf}}. Diese Implementierung orientiert sich an dem offiziell veröffentlichten Quellcode und wird im folgenden Kapitel schrittweise im Detail erklärt. Die Unterschiede zu den Beschreibungen des veröffentlichten Artikels werden in den letzten Abschnitten diese Kapitels ab Seite \pageref{pipeline_changes}  erörtert.  
\\\\
Das DELF-Verfahren lässt sich in vier Phasen einteilen (siehe Abb. \ref{delf_stages}). Zu Beginn steht das sogenannte Fine-Tuning. Hierbei wird ein vortrainiertes Modell, in unserem Fall ein ResNet-50 Netzwerk, auf einem neuen Datensatz weiter trainiert. Die Domäne der Bilder dieses Datensatzes sollte dabei möglichst nahe der späteren Retrievalaufgabe sein, damit das Modell lernen kann aussagekräftige Deskriptoren für diese Art von Bilden zu berechnen. In der nächsten Phase wird auf dem Modell aufbauend ein Attention-Netzwerk trainiert welches die Güte berechneter Deskriptoren bewertet. In der dritten Phase werden für die Bilder der Datenbank, in der gesucht werden soll Deskriptoren extrahiert. Mit Hilfe des Attention-Netzwerks wird eine Vorauswahl besonders geeigneter Deskriptoren getroffen. In anschließenden Vorverarbeitungsschritte werden die Deskriptoren in den Bildern lokalisiert und in eine kompaktere Form überführt. In der finalen Phase kann DELF aktiv genutzt werden. Es können nun Bilder als Suchanfragen gestellt werden. DELF vergleicht eine Anfrage mit allen Bildern des Datensatzes anhand der vorverarbeiteten Deskriptoren. Potentielle Matches zwischen Deskriptoren werden in einem letzten Schritt geometrisch verifiziert. Das Ergebnis einer Anfrage ist eine Rangliste der ähnlichsten Bilder, sortierte nach der Anzahl verifizierte Deskriptoren-Matches mit dem Anfragebild.
\begin{figure}[H]

\includegraphics[scale=1.0]{delf_stages.pdf}
\caption{Die Phasen des DELF-Verfahrens}
\label{delf_stages}
\end{figure}

\section{ResNet}

Das DELF-Verfahren nutzt zur Erstellung von Deskriptoren ein Residuales Netzwerk (kurz ResNet). Bei der im Jahre 2015 vorgestellten ResNet Architektur \cite{resnet} von He, Zhang et al. handelt es sich um eine der meist genutzten tiefen CNN-Architekturen der aktuellen Forschung. ResNets finden Anwendung in unterschiedlichen Machine Learning Aufgaben, wie Klassifikation, Objektdetektion oder Image Retrieval.\\
Zeiler und Fergus haben gezeigt \cite{extraction_point_meaning}, dass CNNs mit wachsender Netzwerktiefe in der Lage sind komplexere Merkmale zu detektieren. Es scheint daher intuitiv zur Lösung immer komplexerer Aufgaben zunehmend tiefere Netzwerke zu konstruieren. Allerdings stellt sich heraus, dass ab einem gewissen Punkt keine Verbesserungen mehr mit dem bloßen aneinanderreihen von immer mehr Schichten erzielt werden können. Werden zu viele Schichten hinzugefügt kann es sogar passieren, dass die Qualität der Netzwerkvorhersagen abnimmt. Mit dem rasanten Anstieg der Anzahl an Netzwerkparameter wird es immer schwieriger das Netzwerk zu optimieren. Parameter konvergieren deutlich langsamer zu einem Optimum und es gibt mehr lokale Minima in denen ein Netzwerk im Optimierungsprozess stecken bleiben kann. ResNets wirken diesem Problem mit der Einführung sogenannter Skip-Verbindungen entgegen. Hierbei werden zusätzliche Direktverbindungen im Netzwerk geschaffen, bei denen einige Schichten übersprungen werden. Fließt eine Eingabe an den Beginn einer Skip-Verbindung, so wird auf dieser die Identität der Eingabe mitgeführt. Parallel durchläuft die Eingabe die übersprungenen Schichten. Am Ausgangspunkt der Verbindung wird schließlich die Ausgabe der übersprungenen Schichten mit der Identität summiert (siehe Abb.~\ref{resnet}a). Durch die Bereitstellung der Identität hat das Netzwerk eine bessere Grundlage zur Optimierung und einzelne schlecht optimierte Schichten weniger negative Auswirkung auf die Netzwerkausgabe. Die Autoren stellen fest, dass CNNs bei Verwendung von Skip-Verbindung schneller zu einem Optimum konvergieren und dabei bessere Minima gefunden werden. 
\\
\begin{figure}[h]
\includegraphics[scale=0.70]{resnet-50.pdf}
\caption{Aufbau der ResNet-Architektur (vgl. Fig.2, Fig.3 aus \cite{resnet})}
\label{resnet}
\end{figure}
ResNets können in unterschiedlichen Konfigurationen erstellt werden. Das für DELF verwendete \mbox{ResNet-50} besteht aus 49 faltenden gefolgt von einer vollvernetzen Schicht. Skip-Verbindungen überspringen jeweils drei Schichten. Das Netzwerk kann in vier Blöcke unterteilt werden. Die Größe der einzelnen Featuremaps, der Ausgabe verringert sich nach jedem Block um den Faktor vier, wohingegen die Merkmalstiefe bzw. Anzahl der Featuremaps in der Ausgabe steigt (Siehe Abb.~\ref{resnet}b). In der Implementierung dieser Arbeit wird die von Torchvision zur Verfügung gestellte ResNet-50 Architektur genutzt\footnote{\url{https://github.com/pytorch/vision/blob/c2e8a00885e68ae1200eb6440f540e181d9125de/torchvision/models/resnet.py}, zuletzt besucht 16.06.20}. 


\section{Trainingsdaten}\label{trainingsdata}
Um die Modelle für das DELF-Verfahren zu trainieren wird ein gelabelter Datensatz benötigt. Zum jetzigen Zeitpunkt steht kein solcher Datensatz von historischen Stadtaufnahmen mit ausreichender Größe zur Verfügung. Für diese Arbeit wird daher alternativ auf die Bilder der Google Landmark Challenge V2 \cite{landmarks_v2} zurückgegriffen. Die Bilder entstammen einer Websuche auf der Wikimedia Datenbank\footnote{\url{https://commons.wikimedia.org}, zuletzt besucht 18.06.20} und zeigen Sehenswürdigkeiten aus der ganzen Welt. Der überwiegende Teil (72\%) zeigt dabei menschengemachte Sehenswürdigkeiten, wie Kirchen, Museen oder Häuser. Auch wenn historische Aufnahmen keinen wesentlichen Teil der Bilder ausmachen enthält der Datensatz viele ähnliche Inhalte zu den historischen Datensätzen, die für das Retrieval genutzt werden. Der Trainingssatz der Landmark Challenge ist mit über 4 Millionen Bildern aus über 200k unterschiedlichen Kategorien sehr groß und heterogen. Da bei der Zusammenstellung keine Verifizierung der Bildinhalts durchgeführt wird kommt es häufig vor, dass Bilder in der falschen Kategorie einsortiert sind. Für das DELF-Training wird daher ein bereinigtes Subset des Datensatzes verwendet, welches von Yokoo, Ozaki et al. in Rahmen ihrer Arbeit \cite{landmarks_verified} erstellt wurde. Aus dem bereinigten Datensatz werden die 40 häufigsten Kategorien gewählt und so ein Trainingsdatensatz über 39\,790 gelabelten Bildern erstellt. 

\section{Fine-Tuning}

Das Ziel der ersten Trainingsphase ist es das ResNet Modell so zu optimieren, dass das Modell bei der Verarbeitung eines Bildes Zwischenergebnisse erzeugt, die den Bildinhalt aussagekräftig beschreiben. Dies ist die Voraussetzung, um später leistungsstarke Deskriptoren erstellen zu können. Während der Optimierung versucht das Netzwerk die Klassifikationsaufgabe des Trainingsdatensatzes zu lösen. Zu Beginn werden die Netzwerkparameter dabei nicht zufällig initialisiert. Stattdessen wird ein Vortrainiertes Modell als Ausgangspunkt genutzt. Dieser Ansatz des Transferlernens wird als Fine-Tuning bezeichnet und ist eine gängige Methode, um den Trainingsprozess zu erleichtern. Auch in anderen Image Retrival Systemen \cite{convnet} \cite{siamac_contrastive_loss} wird diese Methode für die Netzwerkoptimierung genutzt. Zeiler und Fergus zeigen in ihren Experimenten (vgl. \cite{extraction_point_meaning} Kapitel 5.2), dass Netzwerke beim Training lernen allgemein nützliche Merkmale zu extrahieren, die sich auf unterschiedliche Datensätze anwenden lassen. Um ein vortrainiertes Netzwerk auf einen neuen Datensatz anzupassen sind daher nur kleine Veränderungen der Netzwerkparameter notwendig.
\\
Als Ausgangspunkt für das DELF-Training wird ein auf ImageNet trainiertes ResNet-50 genutzt. Bei ImageNet handelt es sich um einen sehr große Klassifikationsdatensatz mit 1.4M Bildern aus 1000 sehr unterschiedlichen Kategorien. Durch die Vielfalt an Kategorien eigenen sich auf ImageNet trainierte Netzwerke als Ausgangspunkt für viele Klassifikationsaufgaben. Daher stellen die meisten Machine Learning Frameworks auf ImageNet trainierte Netzwerke zur Verfügung\footnote{\url{https://pytorch.org/docs/stable/torchvision/models.html}, zuletzt besucht 23.06.20}. 
\\
Während dem Training erwartet das Netzwerk quadratische Bilder mit einer Seitenlänge von 224 Pixeln und 3 Farbkanälen als Eingabe. Hierfür werden die Trainingsdaten zunächst quadratisch zugeschnitten und auf $250\times250$ Pixel skaliert. Anschließend wird ein zufälliger $224\times244$ Pixelbereich ausgewählt.
Um das vortrainierte Netzwerk auf dem Trainingssatz weiter zu trainieren muss das Netzwerk so angepasst werden, dass die Ausgaben der letzten Schicht die korrekte Form für die zur Optimierung verwendete Fehlerfunktion hat. Als Fehlerfunktion wird hier die Kreuzentropie berechnet:
\begin{equation}
\text{Kreuzentropie}(Y,\hat{Y}) = -\sum_{c \in C}{Y(c)*\log\hat{Y}(c)}
\end{equation}
$\hat{Y}$ ist hierbei die Verteilung der Klassenwahrscheinlichkeiten, die das Modell für eine Eingabe vorhergesagt hat. $\hat{Y}(c)$ ist die vom Netzwerk bestimmte Wahrscheinlichkeit, mit der die Eingabe der Klasse $c$ zuzuordnen ist. $Y$ beschreibt die tatsächliche Kategorie der Eingabe. $Y$ ist also ein Verteilung, bei der die Wahrscheinlichkeit für jede, bis auf die korrekte Kategorie 0 und für die tatsächliche Klasse 1 ist. Als Ausgabe des Netzwerks wird also ein Vektor der Wahrscheinlichkeitsverteilung erwartet, dessen Dimensionalität der Anzahl der unterschiedlichen Klassen $|C|$ im Datensatz entspricht und dessen Einträge sich auf 1 summieren. \\
Damit die Ausgaben des Netzwerks die richtige Dimensionalität haben wird zunächst die letzte voll verbundene Schicht entfernt. An ihre Stelle tritt eine faltenden Schicht mit $1\times1$ Filtermasken, die eine Merkmalstiefe von $2048$ erwarten, was der Dimensionalität vorangehenden Schicht entspricht (vgl. Abb.~\ref{resnet}b). In der faltenden Schicht werden $|C|$ dieser Filtermasken auf die Eingabe angewendet, wodurch die Ausgabe die gewünschte Dimensionalität erhält.
Um die Netzwerkausgaben in den richtigen Wertebereich zu überführen, werden diese von einer Softmaxfunktion aktiviert, bevor die Fehlerfunktion berechnet wird:
\begin{equation}
\text{Softmax}(\hat{Y'})_{c} = \frac{e^{\hat{Y'_c}}}{\sum_{ j \in C}{e^{\hat{Y'_j}}}} \forall c \in C
\end{equation}
Hierbei ist $\hat{Y'}$ ein Ausgabevektor des Netzwerks und $\hat{Y'_c}$ der Eintrag des Vektors welcher zur Klasse $c$ zugeordnet ist. Der resultierende Vektor kann als Wahrscheinlichkeitsverteilung über die unterschiedlichen Klassen im Bezug zur Eingabe interpretiert werden. Mit den vorgenommenen Modifikationen kann das ResNet Modell auf dem Trainingsdatensatz optimiert werden. Die für das Fine-Tuning und Attention-Training verwendeten Hyperparamter werden im Experimentalteil in Abschnitt \ref{hyperparam} ab Seite \pageref{hyperparam} erläutert.

\section{Attention Training}

DELF erzeugt eine große Anzahl an lokalen Deskriptoren, um Bilder zu beschreiben. Da jeder Deskriptor nur einen Ausschnitt des Originalbildes beschreibt, werden auch Deskriptoren für wenig aussagekräftige Bereiche, wie z.B. Teile des Himmels erstellt. Diese Deskriptoren beanspruchen nicht nur zusätzliche Rechenzeit während des Matchingprozesses sondern können auch zu falsch positiven Matches führen. Ziel der zweiten Trainingsphase ist es daher auf Basis dieser Deskriptoren ein Netzwerk zu trainieren, welches in der Lage ist die Qualität der Deskriptoren zu bewerten und so ungeeignete Kandidaten herauszufiltern. 
Zur Erstellung der Deskriptoren dient das ResNet-50, welches in der ersten Phase trainiert wurde. Als Deskriptoren werden dabei die Ausgaben aus dem dritten ResNet-Blocks genutzt (vgl.~Abb.~\ref{resnet}b). Die Ausgaben haben eine Dimensionalität von $w\times h\times 1024$, wobei $w$ und $h$ abhängig von der Breite und Höhe des Eingabebildes sind. Die einzelnen Koordinaten der $1024$ Featuremaps lassen sich jeweils auf einen Bildbereich in der Eingabe zurückführen. So kann die Ausgabe in $w \times h$ Deskriptoren der Größe $1024$ eingeteilt werden. Wie auch beim Fine-Tuning werden die Bilder für das Training zunächst quadratisch zugeschnitten. Anschließend werden sie auf eine zufällige Seitenlänge zwischen $255$ bis $720$ Pixel skaliert. Die Skalierung hat Einfluss auf die Beschaffenheit der entstehenden Deskriptoren. Durch das zufällige Skalieren der Trainingsbilder lernt das Attention-Netzwerk mit Deskriptoren aus verschiedenen Skalen umzugehen. Während dem Attention-Training werden die Parameter des ResNets nicht mehr verändert. Die Schichten nach dem Extraktionspunkt in Block 3 werden nicht mehr benötigt und können verworfen werden. \\
\begin{figure}[h]
\centering
\includegraphics[scale=0.77]{attention_spaced.pdf}
\caption{Architektur des Attention Trainings}
\label{attention}
\end{figure}
Aufgabe des Attention-Netzwerks ist es für eine Eingabe an Deskriptoren der Form $w\times h\times 1024$ eine einzelne Featuremap der Größe $w\times h$ zu generieren, dessen Werte jeweils einen Deskriptor bewerten. Wichtig ist dabei, dass die Berechnung der einzelnen Attention-Werte nur von den Werte der dazugehörigen Deskriptoren abhängen dürfen. Dies kann durch faltenden Schichten mit $1\times1$-Filtermasken realisiert werden. Die Attention-Einheit besteht aus zwei solcher Schichten, welche die Merkmalstiefe der Deskriptoren sukzessive auf $1$ reduzieren \mbox{(vgl. Abb.~\ref{attention}a).}  
Um die Parameter der Attention-Einheit zu optimieren müssen ihre Ausgaben zur Lösung der Trainingsaufgabe beitragen. Da die Attention-Werte später genutzt werden um zu entscheiden, welche Deskriptoren Einfluss auf die Lösung der Retrievalaufgabe haben ist es sinnvoll sie auch beim Training in einer Form zu nutzen, die den Einfluss der Deskriptoren zur Lösung der Klassifikationsaufgabe reguliert. Die Attention-Werte werden zur Gewichtung der Deskriptoren genutzt und dafür elementweise mit den Featuremaps der Deskriptoren Multipliziert. Anschließend werden die gewichteten Featuremaps zu jeweils einem Wert summiert. Als Ausgabe ergibt sich ein $1024$ dimensionaler Vektor (vgl. Abb.~\ref{attention}b). Abschließend muss die Ausgabe in eine passende Form für die Berechnung der Kreuzentropie gebracht werden. Dies geschieht analog wie im Fine-Tuning durch Verwendung einer $1\times1$-Faltungsschicht und anschließender Softmaxaktivierung der Ausgabe (vgl. \ref{attention}c).


\section{Extraktion und Verarbeitung}

Nachdem das Training der Modelle abgeschlossen ist, kann mit der Extraktion aller benötigten Informationen über den Retrievaldatensatz begonnen werden. Netzwerkparameter werden ab jetzt nicht mehr modifiziert. Schichten und Operationen nach der Attention-Einheit erfüllen daher keine Zweck mehr und können entfernt werden.
\subsection{Multi-Skalen-Extraktion}
Für jedes Bild des Retrievaldatensatzes werden die lokalen Deskriptoren am Extraktionspunkt nach dem dritten ResNet-Block und die dazugehörigen Attention-Werte nach der Attention-Einheit extrahiert. Da die Skalierung des Bildinhalts Einfluss auf die resultierenden Deskriptoren hat, wird für jedes Bild eine Reihen von unterschiedlich skalierten Versionen betrachtet. Dies ist vergleichbar mit der Verwendung des Scale Spaces im SIFT-Verfahren (vgl. Kap.\ref{related_work} Abs.2) und soll zur Invarianz gegenüber Skalierungsoperationen beitragen. Für jedes Bild werden sechs unterschiedlichen Skalen mit Skalierungsfaktoren zwischen $2$ und $\frac{1}{4}$ erstellt, wobei sich benachbarte Skalen um den Faktor $\sqrt{2}$ unterscheiden. 
\subsection{Deskriptorlokalisierung}\label{rf_chapter}
Für den weiteren Verlauf des Verfahrens muss jeder lokale Deskriptor einem Bereich des Eingangsbildes zuordenbar sein. Bei allen verwendeten Schichten des Modells bis zum Extraktionspunkt handelt es sich um Faltungs- oder Poolingschichten. Von welchen Bereichen der Eingabe die Ausgaben dieser Schichten abhängen lässt sich an drei Parametern festmachen. Die Größe der Filtermasken $k$ bestimmt die Größe des Einflussbereiches einzelner Ausgaben. Die Verschiebung der Filtermasken bzw. die Schrittgröße~$s$ bestimmt die Verschiebung zwischen den Einflussbereichen benachbarter Ausgaben. Das Padding $p$ bestimmt die Größe des Pufferbereichs, der der Eingabe hinzugefügt wird, und sorgt so für eine initiale Verschiebung der Einflussbereiche. Das Padding ist im folgenden immer symmetrisch und wird daher an jeder Seite der Eingabe hinzugefügt. Betrachtet man mehrere aufeinanderfolgende Schichten, so lassen sich die Einflüsse dieser Parameter wie folgt rekursiv berechnen:
\begin{align}
\hat{k}_n &= \hat{k}_{n-1} + ((k_n - 1) * \hat{s}_{n-1})
\\
\hat{s}_n &= \hat{s}_{n-1} * s_n
\\
\hat{p}_n &= \hat{p}_{n-1} + (p_n * \hat{s}_{n-1})
\\
\hat{k}_0 &= k_0
\\
\hat{s}_0 &= s_0
\\
\hat{p}_0 &= p_0
\end{align}
Wobei $\hat{k}_n$, $\hat{s}_n$ und $\hat{p}_n$ die Größe der Einflussbereiche, Schrittgröße und Paddinggröße im Bezug zur ursprüngliche Eingabe nach $n$ Schichten repräsentieren. $k_n$, $s_n$ und $p_n$ zeigen die selben Größen für Schicht $n$ im Bezug zur Ausgabe der vorangehenden Schicht. In Abbildung \ref{receptive_field} werden diese Berechnungen exemplarisch erklärt.
\begin{figure}[h]
\centering
\includegraphics[scale=0.51]{rf.pdf}
\caption{Berechnung des Einflussbereichs einzelner Ausgaben in der ursprünglichen Eingabe. Links nach einer Faltungsschicht, Rechts nach zwei Schichten. Graue Bereiche repräsentiert hinzugefügten Puffer.}
\label{receptive_field}
\end{figure}
Berechnet man diese Werte für den Extraktionspunkt nach dem dritten ResNet-Block ergibt sich für jeden lokalen Deskriptor ein quadratischer Einflussbereich $\mathbf{k}$ mit einer Seitenlänge von $267$ Pixeln im Ursprungsbild. Die Verschiebung zwischen Einflussbereichen benachbarter Deskriptoren $\mathbf{s}$ beträgt dabei $16$ Pixel. Das Ursprungbild erhält bis zu dieser Schicht ein effektives Padding $\mathbf{p}$ von $133$ Pixeln in jede Richtung. Aufgrund dieser Werte lassen sich die Einflussbereiche für alle $w \times h$ Deskriptoren, die am Extraktionspunkt anfallen wie folgt berechnen:
\begin{align}
x_{min}(i,j) &= i*\mathbf{s} - \mathbf{p} 
\\
x_{max}(i,j) &= x_{min}(i,j) + \mathbf{k}
\\ 
y_{min}(i,j) &= j*\mathbf{s} - \mathbf{p}
\\ 
y_{max}(i,j) &= y_{min}(i,j) + \mathbf{k} \qquad\text{ wobei } 0\leq i < w \text{ und } 0\leq j < h
\end{align}

\subsection{Deskriptorselektion}\label{selection_chapter}
Im nächsten Verarbeitungsschritt werden die aussagekräftigsten lokalen Deskriptoren auf Basis der Attention-Werte selektiert. Da während der Deskriptorextraktion unterschiedliche skalierte Bildversionen betrachtet wurden steht vor der Selektion ein große Anzahl an Deskriptoren zur Auswahl, die teilweise auch stark überlappende Bildbereiche beschreiben. Das kann dazu führen, dass aus einigen Bildbereichen sehr viele Deskriptoren ausgewählt werden, die zwar individuell viel Information über das Bild bereitstellen, in der Summe aber wenig Nutzen haben, da der gleiche Bildbereich vielfach beschrieben wird. Da die Anzahl der selektierten Deskriptoren pro Bild begrenzt ist, kann dies zu einer Verdrängung von Deskriptoren aus anderen wichtigen Bildbereichen führen. Um dem entgegenzuwirken werden zunächst Deskriptoren, die stark überlappende Bildbereiche beschreiben mittels Non-Maximum-Suppression aussortiert. Als Metrik für die Überlappung wird hierfür die Intersection-over-Union(IoU) zwischen den Einflussbereichen der Deskriptoren berechnet. Also das Verhältnis zwischen überlappenden Flächeninhalt zu gemeinsam eingenommenem Flächeninhalt der Einflussbereiche.
\begin{equation}
\text{IoU}(e_1,e_2) = \frac{e_1 \cap e_2}{e_1 \cup e_2}
\end{equation}
Wobei $e_1$ und $e_2$ Einflussbereiche zweier Deskriptoren sind. Der Non-Maximum-Suppression Algorithmus (siehe Alg. \ref{nms}) erhält als Eingabe eine Liste mit den Einflussbereichen der lokalen Deskriptoren $\mathcal{E}$, die dazugehörigen Attention-Werte $\mathcal{A}$, sowie einen Schwellwert $T$ der maximal tolerierten IoU zwischen den Einflussbereichen der Deskriptoren. Zunächst werden die Kandidaten nach Attention-Wert sortiert. Der Einflussbereich mit dem höchsten Attention-Wert wird ausgewählt und von der Kandidatenliste in die Ergebnisliste geschoben. Anschließend wir die IoU zwischen dem ausgewählten Einflussbereich und den übrigen Einflussbereichen der Kandidatenliste berechnet. Kandidaten dessen IoU den Schwellwert $T$ überschreiten werden aus der Kandidatenliste entfernt. Anschließend wird aus der Kandidatenliste erneut der Einflussbereich mit dem nun höchsten Attention-Wert ausgewählt und der Prozess wiederholt, bis die Kandidatenliste leer ist. Abschließend wird die Ergebnisliste mit den ausgewählten Bereichen und dazugehörigen Attention-Werten zurückgegeben.

\begin{algorithm}
\caption{Non-Maximum-Suppression}
\label{nms}
\DontPrintSemicolon
\KwIn{$\mathcal{E} \leftarrow \{e_1,\ldots,e_n\}$, $\mathcal{A} \leftarrow \{a_1,\ldots,a_n\}$, $T$}
$\mathcal{S} \leftarrow \emptyset$ \;
 \While{$\mathcal{E} \neq \emptyset$}{
  $x \leftarrow$ argmax($\mathcal{A}$) \tcp*[f]{Wähle bestbewertete Box }\;
  $currBox \leftarrow e_x$  \;
  $\mathcal{E} \leftarrow \mathcal{E}\, \backslash \{e_x\}$ \tcp*[f]{Verschiebe Box von Kandidaten in Ergebnisliste} \;
  $\mathcal{A} \leftarrow \mathcal{A}\, \backslash \{a_x\}$ \;
  $\mathcal{S} \leftarrow \mathcal{S} \cup \{(e_x,a_x)\}$ \;
  \ForEach{$e_i \in \mathcal{E}$}
  {\tcp*[f]{Entferne Boxen mit IoU > T zu currBox aus Kandidatenliste} \;
   \If{$\text{IoU}(e_i,currBox) > T$}{ 
   $\mathcal{E} \leftarrow \mathcal{E}\, \backslash \{e_i\}$ \;
   $\mathcal{A} \leftarrow \mathcal{A}\, \backslash \{a_i\}$ \;
   }
   }
 }
 \Return{$\mathcal{S}$}

\end{algorithm}
Im DELF-Verfahren wird für die Vorsortierung ein IoU-Schwellwert von $0.8$ genutzt. Nach der Vorsortierung findet die finale Selektion der Deskriptoren statt. Für jedes Bild werden hierbei von den verbliebenen Deskriptoren die 1000 Deskriptoren mit den höchsten Attention-Werten ausgewählt.

\subsection{Datentransformation und Dimensionsreduktion}

Ziel des letzten Vorverarbeitungsschritts ist es die Daten der lokalen Deskriptoren in eine Form zu bringen, die ein effizientes Vergleichen möglich macht. Obwohl durch den Selektionsprozess bereits die Anzahl an lokalen Deskriptoren je Bild drastisch reduziert wurde ist die aktuelle Repräsentation der einzelnen Bilder noch sehr groß. Mit $1000$ Deskriptoren mit je $1024$ Dimensionen umfasst die Beschreibung jedes Bildes über eine Millionen Werte. Durch die folgenden Transformationen, wird die Größe der lokalen Deskriptoren und damit der gesamt Repräsentation stark reduziert. Die für DELF beschriebenen Transformationen bezeichnen die Autoren dabei als "common practice" (vgl. \cite{delf} Kap.4.3) und beziehen sich dabei auf das von Jégou und Chum in \cite{common_practice} untersuchte Vorgehen zur Transformation von VLAD und BOW-Deskriptoren. Tatsächlich werden Deskriptoren auch in anderen Image Retrieval Verfahren \cite{convnet} \cite{one} auf ähnliche Weise verarbeitet.
\\
Zunächst werden die Deskriptoren der Länge nach normiert. Anschließend wird auf ihnen eine Hauptkomponentenanalyse durchgeführt. Die resultierende Transformationsmatrix soll dabei repräsentative für die Deskriptoren des gesamten Datensatzes sein, daher werden bei der Analyse Deskriptoren des ganzen Datensatzes oder zumindest eines erheblichen Teils betrachtet. Ziel der Hauptkomponentenanalyse ist es die Daten anhand neuer Dimensionen, den sogenannten Hauptkomponenten, darzustellen. Die Richtungen der Hauptkomponenten ergeben sich aus einer Linearkombination der bisherigen Dimensionen und sind so gewählt, dass für die transformierten Daten keine Korrelation zwischen den Dimensionen besteht. Eine weitere Eigenschaft dieser Darstellung ist, dass die einzelnen Hauptkomponenten jeweils den größtmöglichen Anteil, der in den Daten vorhandenen Varianz abbilden. D.h. die Hauptkomponente, die den größten Teil der Varianz abbildet ist die Richtung entlang welcher die Daten am stärksten Variieren. Die Hauptkomponente, die den nächst größten Anteil der Varianz erklärt beschreibt den größtmöglichen Anteil der verbleibenden Varianz. Dies führt dazu, dass der wesentliche Teil der Varianz von wenigen Dimensionen erklärt wird. Dadurch ist es möglich einen großen Teil der Dimensionen zu entfernen, ohne einen starken Informationsverlust zu erleiden.
\\
Sei $X_{n\times d}$ eine Matrix von $n$ zu analysierenden Deskriptoren mit je $d$ Dimensionen. 
Zunächst werden die Daten zentriert, sodass ihr Mittelwert entlang der $d$ Dimensionen $0$ ist:
\begin{equation}
x'_{i,j} = x_{i,j}-\frac{1}{n}\sum_{k=1}^n{x_{k,j}}\qquad \text{für }0<i\leq n\text{, }0<j\leq d
\end{equation}

Häufig werden die Daten anschließend durch die Standardabweichung der einzelnen Dimensionen geteilt um die Varianz je Dimension auf 1 zu setzten. Je größer die Varianz in den Dimensionen ist, desto größer ihr Anteil in den berechneten Hauptkomponenten. Diese Standardisierung wird daher durchgeführt, wenn die Größe der Varianzen der ursprünglichen Dimensionen nicht adäquat ihre Bedeutsamkeit widerspiegeln. Für DELF wird keine Anpassung der Varianz durchgeführt.
\\
Nach der Hauptkomponentenanalyse werden die Daten in den Raum der Hauptkomponenten transformiert. Die neuen Dimensionen sind dabei nach der von ihnen erklärten Varianz sortiert (vgl. Abb.~\ref{pca_ex}ab). DELF erhält von den transformierten Deskriptoren nur die ersten 40 Dimensionen und erzeugt damit eine deutlich kompaktere Repräsentation der Bilder. Wie sich die Anzahl erhaltener Dimensionen auf das Retrievalergebnis auswirkt wird in Kapitel \ref{pca_experiments} auf Seite \pageref{pca_experiments} experimentell untersucht. Weiterhin sieht das Verfahren vor die Deskriptorendaten zu weißen, d.h. in eine Form zu bringen, in der zwischen den Dimensionen keine Korrelation herrscht und die Varianz entlang der Dimensionen jeweils 1 beträgt (vgl. Abb.~\ref{pca_ex}c). Die erste Voraussetzung ist dabei durch die Transformation in den Raum der Hauptkomponenten automatisch erfüllt. Für die Anpassung der Varianz müssen die Daten durch die Standardabweichungen der neuen Dimensionen geteilt werden. Bevor die Deskriptoren für das Matching genutzt werden, werden sie erneut der Länge nach normiert.
%
\comment{
\\
Im nächsten Schritt wird für die zentrierten Daten $X'_{n \times d}$ die Kovarianzmatrix $C$ berechnet. 
\begin{equation}
C_{d\times d} =  \frac{X^\intercal_{d \times n} * X_{n \times d}}{n}
\end{equation}
Diese lässt sich über Eingenwertzerlegung in ihre Eingenvektoren und Eigenwerte zerlegen.
\begin{equation}
C_{d\times d} = U_{d\times d}\Lambda_{d\times d}U_{d\times d}^\intercal
\end{equation}
$U$ enthält dabei in ihren Spalten die Eigenvektoren von $C$. $\Lambda$ ist eine Diagonalmatrix, die die dazugehörigen Eigenwerte enthält. Die Eigenvektoren geben die Richtungen der Hauptkomponenten an und die dazugehörigen Eigenwerte die Varianz, die durch dich jeweiligen Hauptkomponenten erklärt werden. Sortiert man die Eigenvektoren anhand ihrer Eigenwerte erhält man eine Transformationsmatrix $U'$, mit der sich die Daten so in den Raum der Hauptkomponenten überführen lassen, dass die Dimensionen nach der von ihnen erklärten Varianz sortiert sind (vgl. Abb.~\ref{pca_ex}ab). 
\begin{equation}
\hat{X}_{n \times d} = X'_{n \times d} * U'_{d \times d}
\end{equation}
}
\begin{figure}
\centering
\includegraphics[scale=0.53]{pca.pdf}
\caption{Beispiel zur Hauptkomponentenanalyse auf 2D-Daten}
\label{pca_ex}
\end{figure}
\comment{
Verkürzt man die Transformationsmatrix auf die ersten $k$ Spalten, so werden die Daten in einen $k$-dimensionalen Raum abgebildet.
\begin{equation}
\hat{X}_{n \times k} = X'_{n \times d} * U'_{d \times k}
\end{equation}
In der DELF-Pipeline werden standardmäßig 40 Hauptkomponenten erhalten. In Kapitel \ref{pca_experiments} auf Seite \pageref{pca_experiments} wird dieser Parameter experimentell untersucht. 
DELF sieht es vor die Deskriptordaten ebenfalls zu weißen, d.h. in eine Form zu bringen, in der zwischen den Dimensionen keine Korrelation herrscht und die Varianz entlang der Dimensionen je 1 beträgt (vgl. Abb.~\ref{pca_ex}c). Die erste Voraussetzung ist dabei durch die Hauptkomponentenanalyse automatisch erfüllt. Für die Anpassung der Varianz müssen die Daten durch die Standardabweichungen der Dimensionen geteilt werden. Dies kann wie Folgt durch Nutzung der Eigenwerte erreicht werden:
\begin{equation}
\hat{X}_{white} = \Lambda^{-1/2}\hat{X}
\end{equation}
Abschließend werden die transformierten Deskriptoren erneut L2-normiert.}

\section{Matching}

Sobald die Daten der lokalen Deskriptoren des Datensatzes erhoben, verarbeitet und in den Bildern lokalisiert sind ist das DELF-System in der Lage Suchanfragen zu bearbeiten. Wird ein Bild als Anfrage an das System gestellt werden auch für dieses Bild alle notwendigen Informationen berechnet, wie im letzten Abschnitt beschrieben. Um die Anfrage zu beantworten müssen die Bilder im Suchdatensatz anhand ihrer Ähnlichkeit zum Anfragebild sortiert werden. Hierfür wird jede mögliche Kombination an Bildpaaren aus der Anfrage und dem Suchdatensatz einzeln untersucht. Zunächst wird dabei ein initiales Matching zwischen den Deskriptoren des Anfragebildes $D_\mathcal{A}$ und den Deskriptoren des aktuell betrachteten Bildes $D_\mathcal{S}$ aus dem Suchdatensatz hergestellt. Für jeden lokalen Deskriptor $d_{\mathcal{A}i}$ wird dabei der ähnlichste Deskriptor in $D_\mathcal{S}$ gesucht\footnote{In der offiziellen Implementierung wird für die effiziente Suche der ähnlichsten Deskriptoren ein k-d-Baum \cite{kd_tree} über die Deskriptoren des Anfragebildes erstellt. Eine Effizienzsteigerung mit k-d-Bäumen lässt sich jedoch nur erzielen, solang ein angemessenes Verhältnis zwischen der Anzahl der Datenpunkte $n$ und Dimensionalität $d$ der Daten besteht ($n >> 2^d$)\cite{curse_of_dim}, was in diesem Anwendungsfall nicht gegeben ist.}. Als Metrik wird hierfür die euklidische Distanz berechnet. Ist die Distanz zwischen $d_{\mathcal{A}i}$ und dem ähnlichsten Deskriptor in $D_\mathcal{S}$ kleiner als ein Schwellwert $T$ werden die Deskriptoren als potentielles Match vermerkt (vgl. Alg. \ref{descmatch}).
DELF nutzt für $T$ standardmäßig einen Wert vom $0.8$.

\begin{algorithm}[h]
\caption{Initiales Deskriptor Matching}
\label{descmatch}
\DontPrintSemicolon
\KwIn{$D_\mathcal{A} \leftarrow \{d_{\mathcal{A}1},\ldots,d_{\mathcal{A}n}\}$, $D_\mathcal{S} \leftarrow \{d_{\mathcal{S}1},\ldots,d_{\mathcal{S}n}\}$, $T$}
$\mathcal{M} \leftarrow \emptyset$ \;
 \ForEach(\tcp*[f]{Für jeden Deskriptor in $\mathcal{A}$}){$d_{\mathcal{A}i} \in D_\mathcal{A}$}{
	$closestDistance \leftarrow \inf$ \;
	$closestDescriptor \leftarrow -1$ \;
  \ForEach(\tcp*[f]{Finde ähnlichsten Deskriptor in $\mathcal{S}$}){$d_{\mathcal{S}j} \in D_\mathcal{S}$}{ 
   	\If{$||d_{\mathcal{A}i} - d_{\mathcal{S}j}||_2 < closestDistance$}{ 
   	$closestDistance \leftarrow ||d_{\mathcal{A}i},d_{\mathcal{S}j}||_2$\;
   	$closestDescriptor \leftarrow j$
   }
   }
  \If(\tcp*[f]{Falls näher als $T$}){$closestDistance < T$}{
  $\mathcal{M} \leftarrow \mathcal{M}\cup \{(i,closestDescriptor)\}$ \tcp*[f]{Füge potentielles Match hinzu}
  }
 }
\Return{$\mathcal{S}$}
\end{algorithm}
Anschließend wird überprüft, ob sich die gefundenen Matches auch geometrisch erklären lassen. Die Annahme dabei ist, dass es eine affine Transformation vom Anfragebild auf das betrachtete Bild der Suchdatenbank geben muss, welche die Deskriptorpositionen im Anfragebild auf die Positionen der gefundenen Matches im Suchbild projiziert, falls das betrachtete Bildpaar tatsächlich den gleichen Bildinhalt zeigt.
\\
Für die geometrische Verifikation müssen die Deskriptoren der Matches Koordinaten in ihren dazugehörigen Bildern zugeordnet werden. Hierfür werden die zuvor berechneten Einflussbereiche der Deskriptoren (siehe Kap. \ref{rf_chapter} auf Seite \pageref{rf_chapter}) genutzt. Die Zentren der Einflussbereiche dienen dabei als Referenzpunkte für die Deskirptoren. Für die Verifikation wird der RANSAC-Algorithmus (RANdom SAmple Consesus) \cite{ransac} von Fischler und Bolles genutzt. Als Eingabe erhält der Algorithmus eine Liste mit den Positionen der Deskriptoren aus den potentiellen Matches $\mathcal{L}$, einen Schwellwert $T_{in}$, der festlegt, wie weit die projizierte Positionen maximal von den tatsächlichen Postionen ihres Matches entfernt sein dürfen, um noch von einer Transformation erklärt zu werden und die Anzahl an Versuchen $numTrials$, die unternommen werden um eine passende Transformation zu finden.\\
Zu Beginn werden zufällig Matchpaare aus $\mathcal{L}$ gewählt, mit denen sich eine affine Transformation zwischen den Bildern bestimmen lässt. Für eine affine Ebenentransformation benötigt man drei Koordinatenpaare, um eine Transformationsmatix bestimmen zu können. Nun wird überprüft welche Matches sich von dieser Transformation erklären lassen. Sind die Abstände der transformierten Deskriptorpositionen des Anfragebildes zu ihren Matches im aktuellen Suchbild innerhalb des Schwellwerts $T_{in}$ werden sie als Inlier bezeichnet. D.h. sie können von der Transformation erklärt werden. Dieser Prozess wird mehrfach wiederholt, wobei jedes Mal zufällige Matchpaare für die Berechnung der Transformationsmatrizen genutzt werden.   Nach $numTrails$ Versuchen gibt der Algorithmus die Transformation mit den dazugehörigen Inliern zurück, welche die meisten Matches erklären könnte (vgl. Alg. \ref{ransac}). Für DELF werden standardmäßig 1000 RANSAC-Versuche geeignete Transformationen zu finden. Die Schwellwertdistanz $T_{in}$ beträgt dabei 20. 
\begin{algorithm}[h]
\caption{RANSAC}
\label{ransac}
\DontPrintSemicolon
\KwIn{$\mathcal{L} \leftarrow \{(l_{\mathcal{A}1}, l_{\mathcal{S}1}), \ldots, (l_{\mathcal{A}k}, l_{\mathcal{S}k})\}, T_{in}, numTrials$}
$bestModel \leftarrow \text{None}$ \;
$bestInliers \leftarrow \emptyset$ \;
\For{$i=0, i<numTrials, i$++}{
	\tcp*[l]{Berechne affine Transformation aus zufälligen Matchpaaren}
	$inliers \leftarrow \emptyset$ \;
	$selectedMatches \leftarrow \text{drawRandomMatches}(\mathcal{L}, pairsNeeded=3)$ \;
	$model \leftarrow \text{calcModel}(selctedMatches)$\; 
	\ForEach{$(l_{\mathcal{A}j}, l_{\mathcal{S}j}) \in \mathcal{L}$}
	{
		\tcp*[l]{Sammle von Transformation erklärte Paare}  
		$transformedLocation \leftarrow \text{transform}(l_{\mathcal{A}j}, model)$ \;
		\If{$||l_{\mathcal{S}j} - transformedLocation||_2 < T_{in}$}
		{
		$inliers \leftarrow inliers \cup \{(l_{\mathcal{A}j}, l_{\mathcal{S}j})\}$\;
		}
	}
	\If{$\text{len}(inliers) > \text{len}(bestInliers)$}
	{
		$bestInliers \leftarrow inliers$\;
		$bestModel \leftarrow model$\; 		
	}
}
\Return{$bestInliers, bestModel$} \tcp*[f]{Gib bestes Modell + Inlier zurück}\;
\end{algorithm}
\\
DELF nutzt die Anzahl erklärter Deskriptormatches als Metrik für die Ähnlichkeit zwischen Bildpaaren. So können die Bilder der Suchdatenbank anhand ihrer Ähnlichkeit sortiert und dem Nutzer zurückgegeben werden. In Kapitel \ref{metric_experiment} auf Seite \pageref{metric_experiment} werden Parameter und Metriken zur Bestimmung von Ähnlichkeiten zwischen Bilder experimentell betrachtet. 


\section{Verfahrensunterschiede im DELF Artikel}\label{pipeline_changes}
Die Verfahrensbeschreibung aus dem zu DELF erschienen Artikel (vgl. \cite{delf} Kap.4 und Kap 5.1) weist einige Unterschiede zu dem dazugehörigen veröffentlichten Quellcode auf. Teilweise sind diese Unterschiede der Kürze des Artikels geschuldet. Verfahrensschritte werden meist nur oberflächlich beschrieben, was einen gewissen Interpretationsspielraum für die tatsächliche Umsetzung zulässt. Einige Schritte, wie beispielsweise die Verwendung von  Non-Maximum-Suppression zur Vorsortierung bei der Auswahl der lokalen Deskriptoren (siehe Kap. \ref{selection_chapter} auf Seite \pageref{selection_chapter}), die für das generelle Verständnis des Verfahrens nicht essentiell sind, werden im Artikel nicht erwähnt.
\\
Wesentliche Unterschiede gibt es in der Strukturierung der extrahierten Deskriptoren des Suchdatensatzes, sowie in der Verarbeitung von Suchanfragen. Das im Artikel beschriebene Vorgehen nutzt dabei einen invertierten Index in Kombination mit Produkt Quantisierung \cite{pq}, um lokale Deskriptoren effizient zu matchen. Dieser Ansatz ist insbesondere für die Suche auf sehr großen Datensätzen besser geeignet, als das Vorgehen im veröffentlichten Quellcode. Für die vergleichsweise überschaubaren historischen Datensätze, mit denen sich diese Arbeit beschäftigt stellt dies jedoch kein Problem dar. 
\\
Im Artikel werden beim beantworten einer Anfrage jeweils nicht nur die Deskriptoren eines Bildpaares, sonder alle Deskriptoren des Suchdatensatzes betrachtet. Zunächst wird ein invertierter Index mit einem Codebuch von 8k verschiedenen Visuellen Wörtern über den lokalen Deskriptoren erstellt. Das heißt die lokalen Deskriptoren werden mit Hilfe des K-Means-Algorithmus\cite{k_means} jeweils einem der 8k Clusterzentren zugeordnet. In der zum Cluster gehörigen Postingliste wird dann ein neuer Eintrag erstellt, der Informationen über die Beschaffenheit des Deskriptors und des Bildes, aus dem er extrahiert wurde enthält. Hierfür wird das Residuum $r(x)$ zwischen dem betrachteten Deskriptor $x$ und dem ihm zugeordneten Cluster-Zentrum $q(x)$ berechnet.
\begin{equation}
r(x) = x - q(x)
\end{equation}
Der 40-dimensionale Residuenvektor wird anschließend mit Hilfe eines Produkt Quantisierers auf einen 50 bit Code abgebildet. Der Vektor wird dafür in $m$ Subvektoren aufgeteilt. Die Subvektoren werden dann mittels K-Means-Algorithmus einem von $k$ Clusterzentren zugeordnet. Der Code ergibt sich aus den konkatenierten Indizes der zugeordneten Clusterzentren. Im Fall von DELF werden die Residuenvektoren in 10 Subvektoren zerlegt, die jeweils einem von $2^5$ Clusterzentren zugeordnet werden. Der erzeugte Code bildet die Information über den Deskriptor, welche in den Postinglisten hinterlegt wird.
Bei der Verarbeitung einer Anfrage werden die lokalen Deskriptoren der Anfrage zunächst wieder quantisiert und Postinglisten zugeordnet. Anschließend werden ihre Residuenvektoren mit den Einträgen der Postinglisten verglichen. Hierfür werden die 50 bit Codes wieder über ihre zugehörigen Clusterzentren repräsentiert und die Euklidische Distanz der Zentren zum Residuenvektor des Anfragedeskriptors berechnet (vgl. Abb. \ref{inv_ind}). Auf diese Weise findet das System für jeden lokalen Anfragedeskriptor die nächsten 60 Deskriptoren aus dem Suchdatensatz. Diese Matches werden pro Bild akkumuliert. Anschließend findet eine geometrische Verifikation der Matches mit Hilfe des RANSAC-Algorithmus statt. Die Anzahl an verifizierten Matches bilden auch hier die Grundlage für die Bewertung der Ähnlichkeit zwischen Bildern.
\begin{figure}[h]
\centering
\includegraphics[scale=0.87]{inv_ind.pdf}
\caption{Invertierter Index und Produkt Quantisierung zur Anfrageverarbeitung (vgl. \cite{pq} Fig.5)}
\label{inv_ind}
\end{figure}