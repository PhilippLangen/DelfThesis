\chapter{Parameter Analyse}

Ein wesentliches Ziel dieser Arbeit ist es das Delf-Verfahren insbesondere für den Anwendungsfall des Retrievals von historischen Abbildungen zu optimieren. Hierfür werden ein Reihen an Parametern entlang der Delf-Pipeline variiert und ihre Einflüsse, auf die Retrievalergebnisse analysiert.
Die benötigte Rechenleistung zur Durchführung der Experimente wird freundlicherweise vom 
Zentrum für Informationsdienste und Hochleistungsrechnen (ZIH) an der TU Dresden zur Verfügung gestellt. Das verwendete HPC-DA System\footnote{\url{https://tu-dresden.de/zih/hochleistungsrechnen/hpc}, zuletzt besucht am 28.07.20} ermöglicht es viele Experimente parallel und effizient durchzuführen, sodass auch im zeitlich überschaubaren Rahmen dieser Arbeit eine große Anzahl unterschiedlicher Konfigurationen getestet werden können.

\section{Hyperparameter Optimierung der Trainingsphasen}
Die ersten Experimentalreihen befassen sich mit den Trainingsphasen des Delf-Verfahrens. Um bei späteren Retrievalversuchen gute Ergebnisse erzielen zu können, benötigt man Modelle die in der Lage sind aussagekräftige Bildrepräsentationen zu erstellen. Für die Experimente zum Modelltraining wird dabei angenommen, dass die Güte, der von einem Modell erzeugten Deskriptoren bzw. der von einem Modell getroffenen Auswahl an Deskriptoren positiv mit der Fähigkeit, der Modelle korreliert die beim Training gestellte Klassifikationsaufgaben zu lösen. Um die Modelle zu bewerten wird daher der auftretende Fehler, in Form der Kreuzentropie betrachtet, der auftritt wenn das Modell einen Validierungsdatensatz klassifiziert. Vor Beginn jedes Experiments werden zufällig $20\%$ der Trainingsbilder zum Validierungsdatensatz erklärt. Die Validierungsdaten stehen dem Modell während des Trainingsprozesses nicht zur Verfügung. Daher können sie genutzt werden, um zu überprüfen, ob ein Modell auch auf ungesehenen Daten vergleichbare Ergebnisse erzielt.
\\
Der Erfolg des Trainingsprozesses hängt wesentlich von der zu trainierenden Architektur und der vorhandenen Datenlage ab. Einfluss haben außerdem Parameter, die den Ablauf des Trainingsprozesses beeinflussen. Die Experiments, die in diesem Abschnitt besprochen werden befassen sich mit der Suche nach optimalen Werten für vier dieser sogenannten Hyperparameter. Betrachtet wird die Anzahl der Trainingsepochen, die Lernrate, die bestimmt wie stark Netzwerkparameter in einem Optimierungsschritt angepasst werden, sowie der Faktor $\gamma$ um den die Lernrate alle $10$ Epochen reduziert wird\footnote{Als weiterer Hyperparameter wurde weight decay untersucht, allerdings lies sich hier kein signifikanter Einfluss auf den Validierungsfehler feststellen. Die Ergebnisse hierzu finden sich im Anhang ab Seite \pageref{weight_decay}}.
%wie groß ist unsere suchraum

%was ist nnopt

%nicht betrachtet werden: batch size 8 und sgd zur optimierung

%Ergebnisse
%Generell kann das Resnet die gestellten Aufgaben gut lösen. Das ganze funktioniert mit unterschiedlichen parametern noch sehr gut, daher legen wir keinen sehr großen fokus auf die hyperparamter.
%Finetuning funktioniert gut. Im Finetuningteil ist die Performance schon ob der ersten epoche sehr gut, was nahelegt, dass die netzwerkparameter nur noch wenig angepasst werden (müssen).
% Im Attention training ist schwieriger, da ein teil neuinitialisiert wird. Das obwohl die anzahl der optimierbaren parameter deutlich geringer ist.
% Da bei den meisten runs bereits sehr gute ergebnisse erzielt wurden und nicht mehr viel potential zur verbesserung besteht wurden nicht noch mehr experimente durchgeführt. In Anbetracht der Anzahl an Trails gegenüber den mögliche Suchraums lassen sich keine eindeutlgen/belastbaren Beziehungen gegenüber den hyperparametern festlegen. Es zeigen sich jedoch einige Tendenzen.
% mehr Epochen hat mehr potential
% Lernrate gibt es für attentiontraining einen sweetspot auch sonst tendenzen. 
% Beim Finetuning auch eher niedrige besser oder mit hohem gamma, damit die lernrate sich später einkriegt
% Ist die Lernrate sehr/zu hoch ist das trainingsverhalten insgesamt instabiler. Dazu mehrer lineplots vereinen
 