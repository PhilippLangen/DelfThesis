\chapter{Evaluation}

Die Untersuchung des DELF-Verfahrens lässt sich in zwei wesentliche Abschnitte unterteilen. Zunächst werden in breit angelegten Experimentalreihen unterschiedliche Parameter innerhalb der DELF-Pipeline variiert, um ihren Einfluss auf die Ergebnisse der gelösten Retrievalaufgaben zu quantifizieren. Ziel ist dabei sowohl ein besseres Verständnis für die Einflüsse einzelner Parameter zu schaffen wie auch eine optimale Konfiguration für die Lösung von Retrievalaufgaben speziell für historische Bilder zu finden. \\
Im zweiten Abschnitt der Evaluation werden die Entscheidungen des optimierten DELF-Modells qualitativ untersucht, um ein besseres Verständnis für die Entscheidungsfindung des DELF-Prozesses zu erlangen. Insbesondere wird hier DELF mit anderen Retrievalverfahren verglichen, um Schwächen und Stärken des Verfahrens aufzudecken. \\
Eine Untersuchung zur Reproduzierbarkeit der Ergebnisse aus dem DELF-Papier \cite{delf} findet in der vorliegenden Arbeit nicht statt, da die Autoren für mehreren Parameter innerhalb der DELF-Pipeline keine Angaben zu verwendeten Werten machen. Der von den Autoren verwendete Datensatz für das Training ist außerdem um ein vielfaches größer als der in der vorliegenden Arbeit verwendete Trainingssatz. Ein Training auf so vielen Daten ist im Rahmen der vorliegenden Arbeit nicht umsetzbar. Weiterhin fehlen Informationen, wie die Autoren den von ihnen betrachteten Benchmark-Datensatz (Oxford5k) evaluiert haben. So wird zum Beispiel nicht erläutert, wie mit den sogenannten Störbildern des Datensatzes umgegangen wird\footnote{Mehr Informationen zu Störbildern finden sich im letzten Absatz des folgenden Subkapitels.}, was einen Vergleich von Retrivalergebnissen unmöglich macht.
\section{Evaluationsdaten}

Zur Bewertung von Retrievalsystemen auf historischen Bildern steht ein eigens erstellter Datensatz zur Verfügung (vgl. Tab. \ref{hist4d_data}). Die 848 zusammengestellten Bilder umfassen historische Abbildungen von sieben Dresdner Sehenswürdigkeiten und entstammen vorwiegend den Archiven der Deutschen Fotothek\footnote{\url{https://www.slub-dresden.de/sammlungen/deutsche-fotothek/}, zuletzt besucht am 02.09.20}. Auf Grund der geographischen Nähe einiger Sehenswürdigkeiten sind auf manchen Bildern des Datensatzes mehrere dieser Objekte zu sehen. Bilder sollten von einem Retrievalsystem immer dann zurückgegeben werden, wenn sie mindestens ein Objekt enthalten, das auch im Anfragebild zu sehen ist. Bei den Anfragebildern wird darauf geachtet, dass sie immer genau eines der sieben Sehenswürdigkeiten abbilden.
\\
\begin{table}[h]
\centering

\begin{tabular}{l|c|c|c|c|c|c|c}
\rowcolor[HTML]{C0C0C0} 
Objekt &
  \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Zwinger} &
  \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Hofkirche} &
  \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Frauenkirche} &
  Semperoper \\
Anzahl Objektimpressionen & \multicolumn{2}{c|}{374} & \multicolumn{2}{c|}{216} & \multicolumn{2}{c|}{206} & 89    \\
Anzahl Anfragen           & \multicolumn{2}{c|}{6}   & \multicolumn{2}{c|}{8}   & \multicolumn{2}{c|}{7}   & 7     \\ \hline
\rowcolor[HTML]{C0C0C0} 
Objekt &
  \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Sophienkirche} &
  \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Stallhof} &
  \multicolumn{2}{c|}{\cellcolor[HTML]{C0C0C0}Moritzburg} &
  Total \\
Anzahl Objektimpressionen & \multicolumn{2}{c|}{66}  & \multicolumn{2}{c|}{38}  & \multicolumn{2}{c|}{23}  & 1012  \\
Anzahl Anfragen           & \multicolumn{2}{c|}{6}   & \multicolumn{2}{c|}{4}   & \multicolumn{2}{c|}{4}   & 42    \\ \hline
\rowcolor[HTML]{C0C0C0} 
Impressionen pro Bild     & \hspace{2.5mm} 0 \hspace{2.5mm}         & 1           & \hspace{1.1mm} 2 \hspace{1.1mm}           & 3          & \hspace{2mm} 4 \hspace{2mm}           & 5          & Total \\
Anzahl Bilder             & 0          & 730         & 81          & 29         & 7           & 1          & 848  
\end{tabular}%

\caption{Aufbau des historischen Datensatzes}
\label{hist4d_data}
\end{table}
Vorabexperimente haben gezeigt, dass die von DELF durchgeführte Deskriptorselektion nur befriedigende Ergebnisse liefert, wenn die verwendeten Bilder eine Mindestgröße haben. Im Selektionsschritt wird eine feste Anzahl an Deskriptoren vom Attention-Netzwerk ausgewählt. Da die Anzahl an extrahierten Deskriptoren je Bild proportional zur Bildgröße ist kann es bei sehr kleinen Bildern passieren, dass das Attention-Netzwerk gezwungen ist alle oder einen Großteil der extrahierten Deskriptoren auszuwählen. In diesem Fall findet keine bedeutsame Auswahl an Deskriptoren statt und der positive Effekt des Selektionsprozesses geht verloren (siehe Abb. \ref{small_img}).
\begin{figure}[h]
\includegraphics[scale=0.955]{scale_descriptor_selection.pdf}
\caption{Referenzpunkte der ausgewählten Deskriptoren bei unterschiedlicher Eingangsauflösung.}
\label{small_img}
\end{figure}
Bei sehr großen Bildern können während des Extraktionsprozesses Speicherprobleme auftauchen. Aus diesem Grund werden nur Bilder verarbeitet, die eine Auflösung zwischen $0.6$MPixel und $2.5$MPixel aufweisen. Bilder, die diese Restriktionen über- bzw. unterschreiten, werden vor der Verarbeitung auf die Maximal- bzw. Mindestgröße skaliert.\\
Neben historischen Daten wird das DELF-Verfahren zusätzlich auf dem Oxford5k-Datensatz \cite{oxford5k} getestet (vgl. Tab. \ref{oxford5k_data}). Hierbei handelt es sich um einen häufig verwendeten Benchmarkdatensatz, bestehend aus $5063$ Bildern. Das Bildmaterial entstammt Suchergebnissen zu $11$ unterschiedlichen Sehenswürdigkeiten in und um Oxford, aus der Fotocommunity Flickr\footnote{\url{https://www.flickr.com/}, zuletzt besucht am 03.09.20}. Häufig finden sich dabei Bilder von Personen oder Aufnahmen von Innenräumen, die keine der gesuchten Sehenswürdigkeiten abbilden. Diese Störbilder können entweder als zusätzliche Herausforderung angesehen oder aber vorab aussortiert werden. In den in der vorliegenden Arbeit durchgeführten Experimenten sind diese Störbilder im Datensatz enthalten. 
\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c|c}
\rowcolor[HTML]{C0C0C0} 
Objekt                 & Radcliffe Camera & Christ Church & All Souls   & Magdalen \\
Anzahl Objektimpressionen & 348              & 133           & 111         & 103      \\
Anzahl Anfragen           & 5                & 5             & 5           & 5        \\ \hline
\rowcolor[HTML]{C0C0C0} 
Objekt                & Hertford         & Ashmolean     & Bodleian    & Balliol  \\
Anzahl Objektimpressionen & 61               & 31            & 30          & 18       \\
Anzahl Anfragen           & 5                & 5             & 5           & 5        \\ \hline
\rowcolor[HTML]{C0C0C0} 
Objekt                 & Cornmarket       & Keble         & Pitt Rivers & Total    \\
Anzahl Objektimpressionen & 13               & 11            & 8           & 867      \\
Anzahl Anfragen           & 5                & 5             & 5           & 55       \\ \hline
\rowcolor[HTML]{C0C0C0} 
Impressionen pro Bild     & 0                & 1             & 2           & Total    \\
Anzahl Bilder             & 4218             & 823           & 22          & 5063    
\end{tabular}%
\caption{Aufbau des Oxford5k Datensatzes}
\label{oxford5k_data}
\end{table}

\section{Retrievalmetriken}

Obwohl sich Klassifikations- und Retrievalaufgaben im Kern ähneln, können viele Metriken, mit denen Klassifikationssysteme üblicherweise bewertet werden (z.B. Genauigkeit) nicht verwendet werden, um die Performanz von Retrievalsystemen zu evaluieren. Generell lassen sich aus der Betrachtung einzelner Bildpaare zwischen Anfragebildern und Bildern des Suchindexes keine Rückschlüsse über die Performanz von Retrievalsystemen ziehen. Grundlagen der Bewertung sind stets die gesamten Antworten des Retrievalsystems auf eingehende Suchanfragen. Entscheidend sind hierbei die Rankings bzw. die Reihenfolgen, in denen die Bilder des Suchindexes auf die Anfragen zurückgegeben werden. Eine Möglichkeit, diese Reihenfolgen zu bewerten, ist die Erstellung sogenannter ROC-Kurve (Reciever-Operating-Characteristic, vgl. Abb. \ref{metric_curve}a). Hierfür wird jeweils ein wachsender Anteil der zurückgegeben Bilder betrachtet und das Verhältnis zwischen Richtig-Positiv-Rate bzw. Recall und Falsch-Positiv-Rate dargestellt. Der Recall gibt an, welcher Anteil an Bildern mit gewünschten Bildinhalten bereits im betrachteten Abschnitt der Rückgabe enthalten war.
\begin{equation}
\text{Recall} = \frac{|\text{Bereits züruckgegebene Bilder mit gewünschten Inhalt}|}{|\text{Im Datensatz enthaltene Bilder mit gewünschten Inhalt}|}
\end{equation}
Analog beschreibt die Falsch-Positiv-Rate den Anteil der Bilder ohne gewünschte Bildinhalte, der bereits zurückgegeben wurde.
\begin{equation}
\text{Falsch-Positiv-Rate} = \frac{|\text{Bereits züruckgegebene Bilder ohne gewünschten Inhalt}|}{|\text{Im Datensatz enthaltene Bilder ohne gewünschten Inhalt}|}
\end{equation}
\begin{figure}[h]
\includegraphics[scale=0.76]{metric_curves.pdf}
\caption{Vergleich von Kurvenmetriken zu einem Beispielsranking}
\label{metric_curve}
\end{figure}
\\
ROC-Kurven beginnen stets im Ursprung, wo noch keine Bilder zurückgegeben wurden und enden im Punkt $(1,1)$, wo der gesamte Datensatz zurückgegeben wurde. Die Gerade zwischen diesen Punkten bildet den erwarteten Verlauf, wenn das System Bilder in einer zufälligen Reihenfolge zurückgibt. Ein Retrievalsystem hat nur dann einen positiven Effekt für den Nutzer, wenn die ROC-Kurven seiner Anfrageantworten oberhalb dieser Linie verlaufen. Obwohl ROC-Kurven gut in der Lage sind, den Unterschied eines Suchsystems gegenüber einer zufälligen Suche aufzuzeigen, vermitteln sie aus der Perspektive eines tatsächlichen Anwenders oft ein zu positives Bild der Ergebnisse. Das liegt daran, dass der Anteil der Bilder, die für eine Anfrage relevant sind, meist um ein vielfaches kleiner ist als der Anteil der irrelevanten Bilder, was an einer ROC-Kurve jedoch nicht ablesbar ist. Angenommen, für eine Suchanfrage sind $10\%$ eines durchsuchten Datensatzes tatsächlich relevant und die zur Anfrageantwort erstellte ROC-Kurve zeigt bei einem Recall von $90\%$ eine Falsch-Positiv-Rate von $30\%$. Obwohl der Verlauf der Kurve ein sehr gutes Ergebnis suggeriert, bedeutet dies für den Nutzer, dass $75\%$ der zurückgegebenen Bilder, die er gesehen hat, bevor ein Recall von $90\%$ erreicht wird, nicht den gesuchten Bildinhalt zeigen. Eine Möglichkeit, die tatsächliche Nutzererfahrung besser abzubilden, ist die Erstellung von sogenannten PR-Kurven (Precision-Recall, vgl. Abb. \ref{metric_curve}b). Hierbei wird die Präzision, also der Anteil der relevanten Bildern in der bisherigen Rückgabe, im Verhältnis zum Recall abgebildet.
\begin{equation}
\text{Precision} = \frac{|\text{Bereits züruckgegebene Bilder mit gewünschtem Inhalt}|}{|\text{Bereits züruckgegebene Bilder}|}
\end{equation}
So kann direkt abgelesen werden, welchen Anteil an irrelevanten Bildern innerhalb der Rückgabe toleriert werden müssen, um einen bestimmten Anteil der gesuchten Bilder zu finden.
Die vorgestellten Kurvenmetriken eigenen sich gut, um einzelne Anfragen an Retrievalsysteme zu analysieren und zu visualisieren. Um Retrievalsysteme in Gänze auf Basis mehrerer Anfrage zu bewerten und mit anderen Systemen zu vergleichen, bietet es sich jedoch an, eine kompaktere Metriken zu verwenden. Hierfür können die Flächen unterhalb der Metrikkurven betrachtet werden. AUC (Area Under Curve) und AP (Average Percision) approximieren jeweils die Flächen unterhalb von ROC bzw. PR-Kurven und können so eine Anfrageantwort mit einer einzelnen Zahl bewerten. Auf Grund der besseren Beschreibung des Nutzererlebnisses werden in der vorliegenden Arbeit PR-Kurven bzw. AP-Werte zur Auswertung genutzt. Für den Vergleich zwischen unterschiedlichen Retrievalsystemen oder Konfigurationen von DELF werden die AP-Werte von mehreren Anfragen an ein System zu einem Mittelwert (mAP) zusammengefasst. So kann die Performanz eines Retrievalsystems in einer einzelnen Zahl dargestellt werden. 
 

\section{Parameteranalyse}

Ein wesentliches Ziel der vorliegenden Arbeit ist es, das DELF-Verfahren insbesondere für den Anwendungsfall des Retrievals von historischen Abbildungen zu optimieren. Hierfür werden eine Reihen an Parametern entlang der DELF-Pipeline variiert und ihre Einflüsse auf die Retrieval-Ergebnisse analysiert. Um belastbare Aussagen über die Einflüsse einzelner Parameter machen zu können, muss eine große Anzahl unterschiedlicher Parameterkonfigurationen betrachtet und Experimente diesbezüglich mehrfach wiederholt werden. Die dafür benötigte Rechenleistung ist sehr groß, weshalb sich die Experimente innerhalb des Zeitrahmens einer Masterarbeit nicht auf einem üblichen Desktop-System durchführen lassen.\\
Die Experimente werden daher auf dem HPC-DA System\footnote{\url{https://tu-dresden.de/zih/hochleistungsrechnen/hpc}} des Zentrums für Informationsdienste und Hochleistungsrechnen (ZIH) der TU Dresden durchgeführt. Die verwendete Partition für maschinelles Lernen besteht aus $32$ Knoten, mit jeweils $6$ NVIDIA VOLTA V100 GPUs und $2$ IBM Power9 CPUs mit je $22$ Kernen. Die große Anzahl an leistungsstarken Rechenknoten erlaubt es, viele unterschiedliche Konfigurationen parallel und zügig zu testen.
In Tab. \ref{hpc} findet sich eine Übersicht der rechenintensiven Experimentalreihen, die im Zuge der vorliegenden Arbeit durchgeführt wurden, mit Informationen über verwendete Hardware und benötigter Rechenzeit.

\begin{table}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\rowcolor[HTML]{C0C0C0} 
Experiment                                                                             & \#Läufe & \#CPU-Kerne & \#GPUs & Totale Laufzeit (h) & Totale CPU-Zeit (h) & Totale GPU-Zeit (h) \\ \hline
\begin{tabular}[c]{@{}l@{}}Hyperparameteroptimiertung\\ Finetuning\end{tabular}        & $1$     & $10$        & $10$   & $16$                & $160$               & $160$               \\ \hline
\begin{tabular}[c]{@{}l@{}}Hyperparameteroptimierung\\ Attention-Training\end{tabular} & $2$     & $48$        & $12$   & $40$                & $1920$              & $480$               \\ \hline
Modelltraining                                                                         & $12$    & $2-3$       & $1$    & $78$                & $226$               & $78$                \\ \hline
Retrievalexperimente                                                                   & $919$   & $30$        & $1$    & $2027$              & $60834$             & $2027$              \\ \hline
\multicolumn{4}{|l|}{Summe}                                                                                             & $2325$              & $63140$             & $2745$              \\ \hline
\end{tabular}%
}
\caption{Übersicht über rechenintensive Experimente, durchgeführt auf der ML-Partition des HPC-DA Systems. Rechenzeiten sind jeweils über alle Läufe einer Experimentalreihe summiert. CPU-Zeit bezieht sich auf die akkumulierte Rechenzeit der CPU-Kerne.}
\label{hpc}
\end{table}

\subsection{Hyperparameteroptimierung der Trainingsphasen}\label{hyperparam}
Die ersten Experimentalreihen befassen sich mit den Trainingsphasen des DELF-Verfahrens. Um bei späteren Retrievalversuchen gute Ergebnisse erzielen zu können, benötigt man Modelle, die in der Lage sind, aussagekräftige Bildrepräsentationen zu erstellen. Für die Experimente zum Modelltraining wird dabei angenommen, dass die Güte der von einem Modell erzeugten Deskriptoren bzw. der von einem Modell getroffenen Auswahl an Deskriptoren positiv mit der Fähigkeit der Modelle korreliert, die beim Training gestellte Klassifikationsaufgaben zu lösen. Um ein Modell zu bewerten, wird daher der Fehler in Form der Kreuzentropie betrachtet, der auftritt, wenn das Modell einen Validierungsdatensatz klassifiziert. Vor Beginn jedes Experiments werden zufällig $20\%$ der Trainingsbilder (siehe Kap. \ref{trainingsdata}, S. \pageref{trainingsdata}) für die Validierung ausgewählt. Die Validierungsdaten stehen dem Modell während des Trainingsprozesses nicht zur Verfügung. Daher können sie genutzt werden, um zu überprüfen, ob ein Modell auch auf ungesehenen Daten vergleichbare Ergebnisse erzielt.
\\
Der Erfolg des Trainingsprozesses hängt wesentlich von der zu trainierenden Architektur und der vorhandenen Datenlage ab. Einfluss haben außerdem Parameter, die den Ablauf des Trainingsprozesses beeinflussen. Die Experimente, die in diesem Abschnitt besprochen werden, befassen sich mit der Suche nach optimalen Werten für drei dieser sogenannten Hyperparameter. Betrachtet wird die Anzahl der Trainingsepochen, die Lernrate, die bestimmt, wie stark Netzwerkparameter in einem Optimierungsschritt angepasst werden, sowie der Faktor $\gamma$, mit dem die Lernrate alle $10$ Epochen multipliziert wird\footnote{Als weiterer Hyperparameter wird der Weight Decay untersucht, allerdings lässt sich hier kein signifikanter Einfluss auf den Validierungsfehler feststellen. Die Ergebnisse hierzu finden sich im Anhang in den Abbildungen \ref{weight_decay_1} und \ref{weight_decay_2} ab Seite \pageref{weight_decay_1}}. Die übrigen Hyperparameter sind für alle Experimente festgesetzt. Die Modelle werden mit einer Batchgröße von $8$ trainiert und mittels Stochastic Gradient Decent (SGD \footnote{\url{https://pytorch.org/docs/stable/optim.html\#torch.optim.SGD}, zuletzt besucht am 30.07.20}) optimiert. Für die zu untersuchenden Hyperparameter werden jeweils Wertebereiche definiert. Die Länge des Trainings kann zwischen $10$ und $40$ Epochen betragen. Die initiale Lernrate wird zwischen $0.01$ und $0.001$ gewählt und der $\gamma$-Faktor liegt im Bereich zwischen $1$ und $0.1$.
\\
Um den Suchraum effizient erkunden zu können, wird das von Norman Koch entwickelte NNOPT-Tool verwendet, um neue Hyperparameterkonfigurationen zu erstellen und auf dem HPC-System zu testen. Das NNOPT-Tool, welches auf dem Hyperopt-Paket \cite{hyperopt} von Bergstra, Yamins und Cox basiert, wählt automatisch Werte für die zu untersuchenden Hyperparameter innerhalb der definierten Wertebereiche aus und startet mit diesen Experimente auf dem Großrechner. Ist ein Experiment abgeschlossen, erhält NNOPT zur Bewertung der Konfiguration den erzielten Validierungsfehler. Neue Parameterkonfigurationen werden bevorzugt in der Nähe von bereits getesteten Konfigurationen erstellt, die gute Ergebnisse erzielt haben. Dies erlaubt es, schneller optimale Hyperparameterwerte zu finden als mit einer zufälligen Suche.
\\
Für das Fine-Tuning wurden auf diese Art $22$ unterschiedliche Konfigurationen getestet. Es lässt sich beobachten, dass fast alle getesteten Konfigurationen Modelle erzeugen, die in der Lage sind die Trainingsaufgabe sehr gut zu lösen. Im Schnitt erzielen die trainierten Modelle eine Klassifikationgenauigkeit von $95.13\%$ auf den Validierungsdaten\footnote{Obwohl zum Vergleich der Konfigurationen der Validierungsfehler berechnet wurde, wird die Modellperformanz im Folgenden über die Validierungsgenauigkeit dargestellt, da diese Metrik intuitiver ist.}, wobei $20$ der $22$ Modelle eine Genauigkeit von über $90\%$ erreichen (vgl. Abb. \ref{finetuning_int_end}b). Da auf Grund der guten Ergebnisse nur wenig Potential für Verbesserung besteht und die beobachtete Varianz der Ergebnisse unterschiedlicher Konfigurationen gering ist, werden keine weiteren Testläufe zur Hyperparameteroptimierung durchgeführt. Es sei jedoch erwähnt, dass sich auf Grund der im Verhältnis zu Größe der Suchraumes geringen Anzahl an Testläufen keine eindeutigen Abhängigkeiten zwischen Hyperparametern und Klassifikationsperformanz ableiten lassen.
\\
Gut beobachten lässt sich der positive Effekt der Nutzung eines vortrainierten Modells zur Initialisierung der Netzwerkparameter. So erreichen alle getesteten Konfigurationen bereits nach der ersten Trainingsepoche eine Validierungsgenauigkeit von über $85\%$ (vgl. Abb. \ref{finetuning_int_end}a). Die initialisierten Parameter müssen nur noch geringfügig angepasst werden, um die neue Trainingsaufgabe zu lösen, weshalb die Modelle von Beginn an gute Ergebnisse erzielen. 
\\
\begin{figure}[h]
\includegraphics[scale=0.750]{NNOPT/init_and_end_perf_finetuning.pdf}
\caption{Erreichte Klassifikationsgenaugikeit nach der ersten bzw. letzten Epoche des Fine-Tunings der getesteten Konfigurationen. Achsenskalierung variiert.}
\label{finetuning_int_end}
\end{figure}
\\
Betrachtet man die Ergebnisse der Testläufe in Kombination mit den dazugehörigen Hyperparametern (siehe Abb. \ref{finetuning_all}), so scheint die Anzahl an Trainingsepochen keinen signifikanten Einfluss auf die erzielte Validierungsgenauigkeit zu haben. Dies deckt sich mit der Annahme, dass sich Netzwerkparameter mittels Fine-Tuning in nur wenigen Epochen optimieren lassen. Betrachtet man den Trainingsverlauf des Fine-Tunings (vgl. Abb. \ref{finetuning_lr_gamma_verlauf}), so stellt man fest, dass für die meisten Konfigurationen nach $10-15$ Epochen keine großen Verbesserungen mehr im Hinblick auf die Validierungsgenauigkeit erzielt werden.  
\begin{figure}[h]
\includegraphics[scale=0.58]{NNOPT/finetuning_all.pdf}
\caption{Parallele Darstellung der getesteten Konfigurationen des Fine-Tunings. Jede Linie repräsentiert eine Konfiguration und die von ihr erzielte Validierungsgenauigkeit.}
\label{finetuning_all}
\end{figure}
\\
Beleuchtet man die in den getesteten Konfigurationen genutzten Lernraten, so stellt sich heraus, dass alle Testläufe mit einer Lernrate unter $0.005$ sehr gute Validierungsgenauigkeiten erreichen. Werden höhere Lernraten genutzt, so unterscheiden sich die erzielten Ergebnisse deutlich stärker. Bezieht man die dazugehörigen $\gamma$-Faktoren mit ein, lässt sich erkennen, dass Konfigurationen mit hoher Lernrate aber niedrigem $\gamma$-Faktor, also mit starker Reduktion der Lernrate während des Trainingsverlaufes, ebenfalls sehr gute Ergebnisse erzielen. Läufe mit hoher Lernrate sowie hohem $\gamma$-Faktor schneiden dagegen eher schlechter ab (siehe Abb. \ref{finetuning_lr_gamma}). Analysiert man die Trainingsverläufe der unterschiedlichen Konfigurationen (siehe Abb.\ref{finetuning_lr_gamma_verlauf}), findet sich eine mögliche Erklärung für dieses Verhalten. Bei niedriger Lernrate nähert sich die Validierungsgenauigkeit ohne starke Einbrüche einem Maximum an. Ist die Lernrate hoch, fluktuiert die Validierungsgenauigkeit jedoch stark. Dies weist darauf hin, dass die Modelle nicht in der Lage sind, ein stabiles Optimum für ihre Parameter zu finden. Hohe Lernraten können dazu führen, dass Parameter bei Optimierungsschritten zu stark verändert werden und so ihr Optimum immer wieder überspringen. Nach Abschluss der ersten $10$ Epochen wird die Lernrate mit dem $\gamma$-Faktor multipliziert. Für Läufe mit hoher initialer Lernrate und niedrigem $\gamma$-Faktor lässt sich ab diesem Punkt ein gleichmäßigerer Lernprozess beobachten. 
\begin{figure}[h]
\includegraphics[scale=0.58]{NNOPT/finetuning_lr_gamma.pdf}
\caption{Betrachtung der genutzten Lernraten und $\gamma$-Faktoren. Grün: Konfigurationen mit niedriger Lernrate $(<0.005)$, Rot: Hohe Lernrate und niedriges $\gamma$ $(<0.65)$, Blau: Hohe Lernrate und hohes $\gamma$}
\label{finetuning_lr_gamma}

\centering
\includegraphics[scale=1]{NNOPT/finetuning_lr_gamma_verlauf.pdf}
\caption{Trainingsverlauf der Fine-Tunings, bei unterschiedlicher Lernrate und $\gamma$. Farbgebung analog zu Abb. \ref{finetuning_lr_gamma}}
\label{finetuning_lr_gamma_verlauf}
\end{figure}
\clearpage
Die Konfiguration mit dem besten Trainingsergebnis nutzt eine Trainingsdauer von $22$ Epoche, bei einer initialen Lernrate von $0.0032$ und einem $\gamma$-Faktor von $0.19$ und erzielt nach der letzten Epoche eine Validierungsgenauigkeit von $97.6\%$.
\\
Das dabei trainierte Modell ist der Ausgangspunkt für die Hyperparameteroptimierung des Attention-Trainings. Analog zu den Experimenten des Fine-Tunings werden mit Hilfe von NNOPT $60$ unterschiedliche Konfigurationen für das Attention-Training getestet. Obwohl das Modell für das Attention-Training durch Entfernung des vierten ResNet-Blocks deutlich verkleinert wird und die Ausgaben des ResNets unter Verwendung des gewichteten Sum-Poolings (vgl. Abb. \ref{attention}, S. \pageref{attention}) sehr restriktiv genutzt werden, sind die Attention-Modelle in fast allen untersuchten Konfigurationen in der Lage, die Klassifikationsaufgaben weiterhin sehr gut zu lösen. So werden in $59$ der $60$ untersuchten Konfigurationen eine abschließende Validierungsgenauigkeit von über $89\%$ erreicht (siehe Abb. \ref{attention_int_end}b). Die durchschnittliche Validierungsgenauigkeit nach der letzten Trainingsepoche beträgt $91.3\%$, was einer durchschnittlichen Verschlechterung von $6.3\%$ gegenüber dem verwendeten fine-getunten ResNet-Modell entspricht. Im Kontrast zum Fine-Tuning variiert die gemessene Validierungsgenauigkeit nach der ersten Trainingsepoche der Attention-Trainings deutlich stärker und ist allgemein niedriger (vgl. Abb. \ref{attention_int_end}a). Da die Parameter der zu optimierenden Attention-Einheit nicht vortrainiert sind und daher zufällig initialisiert werden, sind größere Unterschiede und schlechtere Ergebnisse zu Beginn des Trainings sowie eine längere benötigte Trainingsdauer zu erwarten.
\begin{figure}[h]
\includegraphics[scale=0.75]{NNOPT/init_and_end_perf_attention.pdf}
\caption{Erreichte Klassifikationsgenauigkeit nach der ersten bzw. letzten Epoche des Attention-Trainings der getesteten Konfigurationen. Achsenskalierung variiert.}
\label{attention_int_end}
\end{figure}
\\
Betrachtet man die genutzte Anzahl an Trainingsepochen in Kombination mit der erreichten Validierungsgenauigkeit (siehe Abb. \ref{attention_epochs}), bestätigt sich diese Annahme. Konfigurationen mit einer Trainingsdauer unter $20$ Epochen erzielen in unseren Experimente durchschnittlich eine geringere Validierungsgenauigkeit. Ab mehr als $20$ Epochen schwächt sich der positive Effekt einer längeren Trainingsdauer jedoch deutlich ab.
\\
\begin{figure}[h]
\includegraphics[scale=0.58]{NNOPT/attention_epochs.pdf}
\caption{Betrachtung der gewählten Trainingsdauer. Rot: Über $20$ Epochen, Blau: $20$ oder weniger Epochen}
\label{attention_epochs}
\includegraphics[scale=0.58]{NNOPT/attention_top_runs.pdf}
\caption{Darstellung der Validierungsgenauigkeit nach Trainingsende. Rot: Läufe mit Validierungsgenauigkeit $\geq 93.5\%$, Grau: Läufe mit niedrigerer Validerungsgenauigkeit}
\label{attention_top}
\end{figure}
\clearpage
Für Lernrate und $\gamma$-Faktor lassen sich nur schwer Tendenzen erkennen. Über den gesamten Suchraum dieser Parameter lassen sich sowohl Konfigurationen mit hoher sowie Konfigurationen mit weniger hoher Validierungsgenauigkeiten finden. Die Konfigurationen, welche die höchsten Validierungsgenauigkeiten erreichen (siehe Abb. \ref{attention_top}), nutzen jedoch alle eine Lernrate im Bereich zwischen $0.0025$ und $0.009$. Die $\gamma$-Faktoren dieser Konfigurationen liegen meist unter $0.55$ und die Trainingsdauer über $30$ Epochen.
Die beste gefundene Konfiguration erzielt eine Validierungsgenauigkeit von $94.5\%$ und nutzt dabei eine Lernrate von $0.0078$, einen $\gamma$-Faktor von $0.49$ und eine Trainingslänge von $32$ Epochen. 
Mit Hilfe der ermittelten Hyperparameterwerte der besten gefundenen Konfigurationen für Fine-Tuning und Attention-Training werden abschließend $6$ Modelle trainiert, die in den Retrievalexperimenten zur Extraktion der Deskriptoren genutzt werden. Die Trainingsverläufe dieser Modelle sind in Abb. \ref{optimized_runs} dargestellt.
\begin{figure}[h]
\includegraphics[scale=0.75]{NNOPT/6_model_verlauf}
\caption{Trainingsverläufe mit optimierten Hyperparametern. Gleichefarbige Linen gehören zu gemeinsamen Trainingsläufen. Achsenskalierung variiert.}
\label{optimized_runs}
\end{figure}

\subsection{Variieren der Deskriptorlänge}\label{pca_experiments}

Nach Betrachtung der Trainingsphasen und Erstellung optimierter Modelle können nun Parameter aus den späteren Phasen des Verfahrens untersucht werden.
Ein zentraler Parameter der Extraktions- und Verarbeitungsphase ist die Anzahl an Dimensionen der Deskriptoren nach der Transformation mittels Hauptkomponentenanalyse (vgl. \ref{pca_chapter}, S. \pageref{pca_chapter}). Da die zu einem Suchdatensatz erstellte Repräsentation fast ausschließlich aus den transformierten Deskriptoren besteht, sind diese für den überwiegenden Teil des Speicherbedarfs verantwortlich, der während der aktiven Verwendung des Suchsystems anfällt \footnote{Bei der Suche nach den ähnlichsten Deskriptoren während des Matchings hat die Deskriptorlänge auch einen Einfluss auf die Laufzeit. Da der Rechenaufwand für die geometrische Verifizierung von Matches, der nicht von der Deskriptorlänge abhängt, jedoch deutlich größer ist, ist dieser Einfluss vernachlässigbar.}. Insbesondere bei der Suche in großen Datenbanken ist es daher notwendig, die Deskriptoren stark zu komprimieren. Durch die Komprimierung geht jedoch auch immer ein Teil der ursprünglich in den Deskriptoren enthaltenen Informationen verloren. Es wird erwartet, dass sich der Informationsverlust durch die Deskriptortransformation negativ auf die Qualität des initialen Deskriptormatchings auswirkt. Um diesen Informationsverlust zu quantifizieren, wird der Anteil der ursprünglichen Varianz zwischen den Deskriptoren errechnet, die nach der Transformation erhalten bleibt. Hierbei werden alle Deskriptoren betrachtet, die für die Berechnung der Hauptkomponentenanalyse genutzt werden. Es ist davon auszugehen, dass sich der Informationsverlust bei an der Analyse nicht beteiligten Deskriptoren ähnlich verhält. Die Autoren des DELF-Papiers \cite{delf} schlagen als Kompromiss zwischen erhaltener Information und Kompaktheit vor, die Deskriptoren auf $40$ Dimensionen zu reduzieren. Wie dieser Wert ermittelt wurde, wird jedoch nicht beschrieben. Um eine geeigneten Anzahl an Dimensionen zu ermitteln, werden neben der von den Autoren empfohlenen Anzahl auch die Hälfte bzw. ein Vielfaches an erhaltenen Dimensionen (bis zu $200$) getestet. In Abbildung \ref{explained_variance_ratio} wird der Einfluss der Deskriptorlänge auf den Anteil der erklärten ursprünglichen Varianz dargestellt.
\begin{figure}[h]
\centering
\includegraphics[scale=1]{explained_variance.pdf}
\caption{Erklärter Anteil der Varianz nach Deskriptortransformtion durch Hauptkomponentenanalyse auf unterschiedlichen Retrievaldatensätzen, je nach Anzahl erhaltener Dimensionen.}
\label{explained_variance_ratio}
\end{figure}
Die Variation der Deskriptorlänge hat nicht nur Einfluss darauf, welche Deskriptorpaare während des initialen Matchings bestimmt werden, sondern auch auf die euklidische Distanz zwischen diesen Paaren. Die Deskriptoren werden vor dem Matching zwar längennormiert, sodass der maximale Abstand zwischen Deskriptoren $2$ beträgt. Der mittlere Abstand der bestimmten Deskriptorenpaare ist jedoch von der Varianz zwischen den Deskriptoren abhängig und steigt daher mit wachsender Deskriptorlänge. Während des initialen Deskriptormatchings werden Matchingpaare, deren Abstand über einem Distanzschwellwert liegen, verworfen, um inkorrekte Matches auszusortieren (vgl. Alg. \ref{descmatch}, S. \pageref{descmatch}). In Abb. \ref{influence_dim} lässt sich beobachten, wie sich die Anzahl an gefundenen Deskriptormatches mit steigender Länge der verwendeten Deskriptoren verringert, wenn der gewählte Distanzschwellwert unverändert bleibt. In Abb. \ref{influence_threshold} ist analog bei einer konstanten Deskriptorlänge von $80$ der Einfluss von unterschiedlichen Schwellwerten dargestellt. Je größer der Schwellwert, desto weniger Deskriptormatches werden aussortiert. Die untersuchten Deskriptorlängen werden daher auch mit mehreren unterschiedlichen Schwellwerten getestet, um geeignete Kombinationen zu finden. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.86]{influence_dim}
\caption{Einfluss der Deskriptorlänge auf das Deskriptorenmatching zwischen Anfragebild und Bildern mit und ohne gesuchten Bildinhalten aus dem Datensatz bei einem Schwellwert für maximale Deskriptordistanz von $0.8$. Gelbe Punkte markieren Deskriptoren, die im Partnerbild gematched wurden. Rote Linien markieren geometrisch verifizierte Matches}
\label{influence_dim}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.86]{influence_threshold}
\caption{Einfluss des Distanzschwellwerts auf das Deskriptorenmatching bei Verwendung von $80$ dimensionalen Deskriptoren. Gelbe Punkte markieren Deskriptoren, die im Partnerbild gematched wurden. Rote Linien markieren geometrisch verifizierte Matches}
\label{influence_threshold}
\end{figure}

Die Autoren des DELF-Papiers \cite{delf} verwenden für $40$-dimensionale Deskriptoren einen Schwellwert von $0.8$. 
Im Folgenden werden neben diesem Wert sowohl kleiner als auch größere Werte zwischen $0.6$ und $1.4$ in Intervallschritten von $0.2$ getestet. Alle Parameterkombinationen von Deskriptorlänge und Distanzschwellwert werden auf den $6$ trainierten Modellen getestet. Für die Auswertung werden die Ergebnisse jeweils über die Läufe der $6$ Modelle gemittelt. Zusätzlich werden Fehlerbereiche in Größe der Standardabweichung in beide Richtungen angegeben. Dieses Vorgehen wird auch für alle folgenden Parameteranalysen in der vorliegenden Arbeit angewendet. In Abb. \ref{mAP_num_dim} ist für alle betrachteten Konfigurationen von Deskriptorlänge und Distanzschwellwert, die auf dem historischen Datensatz sowie auf den Oxford5k-Daten getestet werden, die erreichte mAP dargestellt.

\begin{figure}[h]
\includegraphics[scale=0.73]{mAp_num_dim}
\caption{Erreichte mAP bei unterschiedlicher Deskriptorlänge und Distanzschwellwert.}
\label{mAP_num_dim}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.73]{mAp_var_dist_ratio}
\caption{Erreichte mAP bei unterschiedlichen Verhältnissen zwischen erklärtem Varianzanteil und gewähltem Distanzschwellwert.}
\label{mAP_var_dist_ratio}
\end{figure}
Unabhängig von der getesteten Konfiguration erzielt DELF auf dem Oxford5k-Datensatz eine deutlich höhere mAP als auf den historischen Daten. Wie erwartet lassen sich mit größerer Deskriptorlänge bei geeignetem Distanzschwellwert tendenziell bessere Ergebnisse erzielen. Die beste gefundene Konfiguration auf den historischen Daten nutzt Deskriptoren der Länge $160$ bei einem Schwellwert von $1.0$ und erzielt im Mittel eine mAP von $0.57$. Auf Oxford5k erreicht die beste Konfiguration im Mittel eine mAP von $0.71$ und nutzt dabei eine Deskriptorlänge von $120$ mit einen Schwellwert von $1.0$. Allerdings lässt sich auf Oxford5k mit der von den DELF-Autoren empfohlenen Deskriptorlänge von $40$ und dem empfohlenen Schwellwert von $0.8$ mit einer mittleren mAP von $0.70$ ein fast gleich gutes Ergebnis erzielen. Lediglich bei Verwendung von noch kleineren Deskriptoren lassen sich signifikante Performanzeinbußen feststellen. So erreicht die beste Konfiguration mit Deskriptorlänge $20$ im Mittel nur eine mAP von $0.65$. Auf den historischen Daten lassen sich mit Deskriptoren mit mehr als $40$ Dimensionen noch signifikante Verbesserungen erzielen. So erreicht die beste Konfiguration mit $80$ Dimensionen eine im Mittel um $0.03$ höhere mAP als die von den Autoren empfohlene Konfiguration. Eine weitere Vergrößerung der Deskriptoren zieht allerdings auch auf den historischen Daten keine großen Performanzverbesserungen nach sich.
\\
Bei Betrachtung der Distanzschwellwerte stellt man fest, dass der optimale Schwellwert mit der Länge der verwendeten Deskriptoren steigt. Dies deckt sich mit den Beobachtungen aus Abb. \ref{influence_dim} und Abb. \ref{influence_threshold}, da mit wachsender Deskriptorlänge ein höherer Schwellwert gewählt werden muss, um eine adäquate Anzahl an Deskriptormatches zu erhalten. Bei einem zu klein gewählten Schwellwert werden fast alle potentiellen Deskriptormatches aussortiert, was zu einem drastischen Perfomanzverlust führt. Ein zu groß gewählter Schwellwert führt ebenfalls zu Performanzverlusten, welche jedoch deutlich geringer ausfallen. Das ist der zusätzlichen geometrischen Verifikation mittels RANSAC zu verdanken. Selbst wenn durch den Schwellwert keine unerwünschten Deskriptormatches verworfen werden, wird ein Großteil dieser Matches während der geometrischen Verifikation durch keine Transformation erklärt und somit aussortiert\footnote{In Abbildung \ref{mAP_num_dim_scoring_methods} auf Seite \pageref{mAP_num_dim_scoring_methods} wird deutlich, dass die Performanz stark unter einem zu hohen Schwellwert leidet, wenn keine geometrische Verifikation durchgeführt wird.}. \\
Wie bereits erläutert, hängt die Wahl eines geeigneten Schwellwerts stark von der Länge der Deskriptoren bzw. dem Anteil der von ihnen erklärten Varianz ab. Eine mögliche Strategie für die Untersuchung von vielen unterschiedlichen Deskriptorlängen ist daher, den Schwellwert direkt abhängig von der erklärten Varianz zu bestimmen und so die Anzahl zu testender Konfigurationen zu reduzieren. In Abb. \ref{mAP_var_dist_ratio} ist die erreichte mAP im Bezug zum Verhältnis zwischen erklärter Varianz und Schwellwert abgebildet. Tatsächlich erreichen fast alle Deskriptorlängen ihr bestes Ergebnis, wenn das Verhältnis zwischen dem Anteil der erklärten Varianz und dem gewählten Schwellwert zwischen $0.5$ und $0.7$ liegt\footnote{Dies wird besonders deutlich, wenn keine geometrische Verifikation stattfindet und zu große Schwellwerte stärkere negative Einflüsse zeigen (vgl. Abb. \ref{mAP_var_dist_ratio_alternative_scoring}a,b, S. \pageref{mAP_var_dist_ratio_alternative_scoring}).}. Für die Untersuchung anderer Deskriptorlängen wäre es daher sinnvoll, Schwellwerte zu testen, die innerhalb dieses Bereiches liegen.

\subsection{Alternativen zur Bewertung von Deskriptormatches}\label{metric_experiment}

Bei der Überprüfung auf geometrische Plausibilität von Deskriptormatches mittels RANSAC werden durch Betrachtung der Deskriptorpositionen zusätzliche Informationen für das Matching nutzbar gemacht, die idealerweise Suchergebnisse verbessern. Dabei sollte beachtet werden, dass diese geometrische Überprüfung für einen Großteil der für das Matching benötigten Rechenzeit verantwortlich ist. Im Folgenden wird analysiert, ob und wie stark sich Suchergebnisse unter Verwendung von RANSAC gegenüber alternativen Bewertungsmethoden für Deskriptormatches verbessern, um zu bewerten, wie sinnvoll dieser zusätzliche Arbeitsschritt ist.
\\
Eine einfache Möglichkeit, ein Matching ohne geometrische Verifikation zu bewerten, ist es, die Anzahl an Deskriptormatches zu berechnen. Je mehr Deskriptorpaare zwischen einem Bildpaar gefunden werden können, desto mehr sollte sich der Inhalt dieser Bilder ähneln. Voraussetzung für eine aussagekräftige Bewertung ist die Verwendung eines geeigneten Distanzschwellwertes. Dieser entscheidet, welche Deskriptorpaare keine ausreichende Ähnlichkeit vorweisen können und daher verworfen werden sollten. Paare, die diesen Schwellwert nicht überschreiten, gehen alle mit der gleichen Gewichtung in die Bewertung mit ein. 
\\
Intuitiv stellen Deskriptorpaare, die sich besonders stark ähneln, jedoch einen besseren Indikator für die Ähnlichkeit zwischen Bildern dar als Paare die diesen Schwellwert nur knapp unterschreiten. Daher wird als weitere Bewertungsmethode eine gewichtete Anzahl an Deskriptorpaaren berechnet. Paare, die den Schwellwert unterschreiten, gehen hierbei mit einem Gewicht zwischen $0$ und $1$ in die Bewertung ein. Ein Paar an identischen Deskriptoren geht dabei mit Gewicht $1$ und ein Paar, dessen Distanz genau dem Schwellwert entspricht, mit Gewicht $0$ in die Bewertung ein. Die Gewichtung verläuft linear antiproportional zur Distanz der Deskriptorpaare. Die gewichtete Anzahl bei Distanzschwellwert $T$ ergibt sich aus

\begin{equation}
\text{gewichtete Anzahl} = \sum_{i=1}^{|D_\Delta|}{1 - \frac{d_{\Delta i}}{T}},
\end{equation}

wobei $D_\Delta$ die Distanzen aller Deskriptorpaare $d_{\Delta i}$ enthält, die den Schwellwert nicht überschreiten.
\begin{figure}[h]
\includegraphics[scale=0.73]{mAp_num_dim_scoring_methods}
\caption{Erreichte mAP bei unterschiedlicher Deskriptorlänge und Distanzschwellwert unter Verwendung alternativer Bewertungsmethoden.}
\label{mAP_num_dim_scoring_methods}
\end{figure}
In Abb. \ref{mAP_num_dim_scoring_methods} sind die Ergebnisse der alternativen Bewertungsmethoden analog wie für RANSAC in Abb. \ref{mAP_num_dim} dargestellt.\\
Auch mit alternativen Bewertungsmethoden erzielt DELF auf dem Oxford5k-Datensatz eine deutlich höhere mAP als auf den historischen Daten. 
Deskriptoren mit mehr Dimensionen erreichen unter Verwendung eines optimierten Distanzschwellwerts eine tendenziell höhere mAP, wobei die Performanzunterschiede auf den historischen Daten deutlich größer ausfallen als auf dem Oxford5k-Datensatz. Die größte Verbesserung diesbezüglich wird auf den historischen Daten erzielt, wenn die Anzahl an Deskriptormatches zur Bewertung verwendet wird (siehe Abb. \ref{mAP_num_dim_scoring_methods}a). Hier verbessert sich die durchschnittliche mAP bei einer Erhöhung der Deskriptorlänge von $20$ auf $200$  um $0.09$.\\
Die Verwendung eines sehr kleinen Distanzschwellwertes führt, wie bereits unter RANSAC beobachtet (vgl. Abb. \ref{mAP_num_dim}), zu einer drastischen Verschlechterung der Performanz, unabhängig welche Bewertungsmethode genutzt wird. Bei Wahl eines sehr großen Schwellwertes, der unter Verwendung von RANSAC nur zur geringfügigen Verschlechterungen der Performanz führt, werden deutlich schlechtere Ergebnisse erzielt, wenn die Anzahl der Deskriptormatches zur Bewertung genutzt wird (siehe Abb. \ref{mAP_num_dim_scoring_methods}a,b). Je höher der Schwellwert gewählt ist, desto mehr Deskriptorpaare, deren Deskriptoren keinen ähnlichen Bildinhalt repräsentieren, werden akzeptiert. Da fälschlicherweise akzeptierte Deskriptorpaare gleichermaßen in die Bewertung eines Matchings eingehen, können auch Bildpaare ohne ähnlichen Bildinhalt hoch bewertet werden. Ab einem gewissen Punkt kann in jedem Bildpaar für jeden Deskriptor ein Partner gefunden werden. Somit erhält jedes Bildpaar die gleiche Bewertung, wodurch Anfragen mit Bildern in einer zufälligen Reihenfolge beantwortet werden. Werden die Deskriptorpaare nach ihrer Ähnlichkeit gewichtet (siehe Abb. \ref{mAP_num_dim_scoring_methods}c,d), gehen Deskriptorpaare, die sehr unterschiedliche Bildinhalte repräsentierten, deutlich schwächer in die Bewertung ein, weshalb sich ein zu großer Schwellwert kaum negativ auf die Ergebnisse auswirkt.
\\
\begin{figure}[h]
\includegraphics[scale=0.74]{compare_scoring_methods}
\caption{Erreichte mAP bei unterschiedlicher Deskriptorlänge mit optimiertem Distanzschwellwert unter Verwendung unterschiedlicher Bewertungsmethoden.}
\label{compare_scoring_methods}
\end{figure}
Wie sich die Performanz je nach gewählter Bewertungsmethode und Deskriptorlänge unterscheidet, ist in Abbildung \ref{compare_scoring_methods} dargestellt. Die hier gezeigten Konfigurationen verwenden jeweils die besten für sie gefundenen Schwellwerte. Auf den Oxford5k-Daten werden mit RANSAC deutlich bessere Ergebnisse als mit anderen Bewertungsmethoden erzielt. Die beste gefundene Konfiguration erreicht dabei im Mittel eine mAP von $0.71$. Die beste Konfiguration mit einer alternativen Bewertungsmethode nutzt die Anzahl Matches zur Bewertung und erreicht eine mittlere mAP von $0.68$. Die Ergebnisse bei Betrachtung von gewichteten Matches unterscheidet sich dabei kaum von den Ergebnissen basierend auf der reinen Matchanzahl.
\\
Auf den historischen Daten fallen die Verbesserungen durch RANSAC deutlich geringer aus. Insbesondere bei Verwendung von hochdimensionalen Deskriptoren werden durch geometrische Verifikation keine besseren Ergebnisse erzielt als bei Verwendung der Matchanzahl. Die beste gefundene Konfiguration für die historischen Daten nutzt ebenfalls die Anzahl an Matches zur Bewertung und erzielt eine mittlere mAP von $0.57$. Die Unterschiede zur besten RANSAC-Konfiguration liegen jedoch innerhalb der ermittelten Fehlerbereiche. Die Betrachtung anhand gewichteter Matches schneidet auch auf den historischen Daten größtenteils ähnlich wie die Betrachtung anhand der Anzahl an Matches ab. Lediglich bei Verwendung sehr großer Deskriptoren werden im Vergleich schlechtere Ergebnisse erzielt. Speziell für den historischen Anwendungsfall scheint der zusätzliche geometrische Verifikationsschritt zu keinen signifikanten Verbesserungen der Performanz zu führen.
Um genauer einzugrenzen, auf welchen Daten RANSAC in der Lage ist, Retrievalergebnisse zu verbessern, und wo dies nicht funktioniert, ist in Tab. \ref{scoring_mAP_change_by_category} für die unterschiedlichen Anfragekategorien aufgeführt, wie sich die mAP bei Verwendung von RANSAC gegenüber der Bewertung mittels Matchanzahl verändert.
\\
\begin{table}[]
\setlength\tabcolsep{0pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccccccc} 

\large{Kategorie}                          & \large{Frauenkirche}                    & \large{Hofkirche}                       & \large{Zwinger}                         & \large{Sophienkirche} & \large{Semperoper} & \large{Moritzburg} & \large{Stallhof} \\
&\includegraphics[scale=1.4]{category_thumbs/frauenkirche_thumb}&\includegraphics[scale=1.4]{category_thumbs/hofkirche_thumb} &      \includegraphics[scale=1.4]{category_thumbs/zwinger_thumb}&\includegraphics[scale=1.4]{category_thumbs/sophienkirche_thumb}&     \includegraphics[scale=1.4]{category_thumbs/semperoper_thumb}&\includegraphics[scale=1.4]{category_thumbs/moritzburg_thumb}&\includegraphics[scale=1.4]{category_thumbs/stallhof_thumb}                    \\
\large{$\Delta$mAP} & \large{$0.076$} & \large{$0.076$} & \large{$0.025$} & \large{$-0.011$} & \large{$-0.034$} & \large{$-0.074$} & \large{$-0.153$}\\              
\end{tabular}
}
\setlength\tabcolsep{6pt}
\caption{Veränderung der mAP bei Verwendung von RANSAC gegenüber einfachem Zählen der Deskriptormatches, je nach Objektkategorie. Der betrachtete Testlauf verwendet eine Deskriptorlänge von $200$ und einen Distanzschwellwert von $1.0$.}
\label{scoring_mAP_change_by_category}
\end{table}
Dabei zeigt sich, dass Motive mit gut differenzierbaren markanten Bereichen, wie beispielsweise die Türme der Frauenkirche stark von einer geometrischen Verifizierung profitieren. Objekte, die häufig wiederkehrende Elemente enthalten, wie zum Beispiel die Fenster der Moritzburg oder die Arkaden des Stallhofs, verschlechtern sich dagegen bei Verwendung von RANSAC.
\\
\begin{figure}[h]
\includegraphics[scale=0.595]{where_ransac_shines}
\caption{Beispiele für Matchings von Bildpaaren mit gleichem Bildinhalt, die von geometrischer Überprüfung profitieren, bzw. darunter leiden. Der Rückgabeindex gibt an, an wievielter Stelle der Anfrageantwort das entsprechende Bildpaar zurückgegeben wird (niedriger ist besser). Gelbe Punkte repräsentieren Deskriptoren, für die ein Match gefunden wurde. Rote Linen verbinden verifizierte Deskriptorpaare.}
\label{where_ransac_shines}
\end{figure}

Warum die geometrische Überprüfung hier problematisch ist, lässt sich gut an dem Beispielmatching zweier Stallhofbilder in Abb. \ref{where_ransac_shines} b erkennen. Obwohl DELF viele Matchpaare entlang der Arkaden und Fenstern findet, können nur wenige mit einer affinen Transformation erklärt werden. Betrachtet man die verifizierten Matches, so fällt auf, dass DELF Schwierigkeiten hat, die korrekten Arkaden miteinander zu matchen. Auf Grund der sich wiederholenden Strukturen können einzelne Bereiche nicht klar genug unterschieden werden. Das resultierende Matching lässt sich daher nicht einheitlich mit einer Transformation erklären. Im Gegensatz dazu konzentriert sich das Beispielmatching der Frauenkirche (vgl. Abb. \ref{where_ransac_shines} a) nur auf zwei ihrer seitlichen Türme, wodurch sich eine Transformation finden lässt, die einen Großteil der Deskriptorpaare erklärt. Obwohl hier die Anzahl der Deskriptorpaare vergleichsweise gering ist, kann durch die geometrische Verifikation ein gutes Matching erstellt werden.
\\
Abschließend lässt sich zusammenfassen, dass der zusätzliche Verifikationsschritt, den DELF vorschreibt, nicht uneingeschränkt sinnvoll für das Lösen von Retrievalaufgaben ist. Architektonische Besonderheiten und Beschaffenheit der Bildinhalte haben einen signifikanten Einfluss auf Effektivität von RANSAC, somit sollte der Einsatz je nach Datensatz überdacht werden. Um herauszufinden, ob RANSAC allgemein für die historische Domäne ungeeignet ist, sollten jedoch weiter Experimente auf einem erweiterten Datensatz insbesondere mit mehr unterschiedlichen Motiven durchgeführt werden.


\subsection{Alternative Extraktionspunkte}

Das DELF-Verfahren sieht es vor, die Ausgaben des dritten Blocks aus dem zugrundeliegenden ResNet-50 (vgl. Abb. \ref{resnet}b, S. \pageref{resnet}) als Deskriptoren zu verwenden. Warum genau dieser Punkt zur Extraktion der Deskriptoren verwendet wird, begründen die Autoren des DELF-Papiers \cite{delf} jedoch nicht. Als letzte Parameterbetrachtung der vorliegenden Arbeit wird daher ein alternativer Extraktionspunkt untersucht. Dabei werden die Netzwerkausgaben des vierten und letzten ResNet-Blocks betrachtet. Diese Wahl beruht auf den Beobachtungen von Zeiler und Fergus, die in \cite{extraction_point_meaning} gezeigt haben, dass Netzwerkausgaben aus späteren Schichten eines neuronalen Netzes in der Lage sind, komplexere Bildmerkmale detektieren. Deskriptoren aus dieser späten Schicht können daher möglicherweise besser für die Differenzierung komplexer Bildinhalte genutzt werden.
\\
Die bisher verwendeten Extraktionsnetzwerke können für die Deskriptorextraktion aus Block-4 weiter verwendet werden. Da sich die extrahierten Deskriptoren jedoch in Form und Inhalt von Deskriptoren aus Block-3 unterscheiden, müssen für die Deskriptorauswahl neue Attention-Netzwerke trainiert werden. Zunächst findet hierfür eine Optimierung der Hyperparameter analog wie in Kap. \ref{hyperparam} statt. Es zeigt sich dabei, dass alle Testläufe unabhängig ihrer Hyperparameterkonfigurationen ähnlich gute Testergebnisse erzielen\footnote{Die Hyperparameteroptimierung des Attention-Trainings mit Deskriptoren aus Block-4 ist im Anhang ab Seite \pageref{hyperopt_layer4_1} in den Abbildungen \ref{hyperopt_layer4_1} bis \ref{hyperopt_layer4_3} illustriert.}. Mit den optimierten Hyperparametern werden anschließend $6$ neue Attention-Modelle trainiert. Die Trainingsverläufe dieser Modelle (siehe Abb. \ref{optimized_runs_layer4}) erreichen bereits ab der ersten Epoche sehr hohe Validierungsgenauigkeiten und steigern sich während des Trainings nur geringfügig. Im Vergleich dazu zeigen Trainingsverläufe auf Basis von Deskriptoren aus Block-3 eine sehr geringe initiale Validierungsgenauigkeit, die sich während des Trainings deutlich steigert (vgl. Abb. \ref{optimized_runs} b, S. \pageref{optimized_runs}). 
\begin{figure}[h]
\centering
\includegraphics[scale=1]{NNOPT/6_model_verlauf_layer4}
\caption{Verläufe des Attention-Trainings für Deskriptoren aus Block-4 mit optimierten Hyperparametern.}
\label{optimized_runs_layer4}
\end{figure}
Die Ausgaben aus dem vierten Block gehen während des Fine-Tunings nach einem finalen Pooling direkt in die Klassifikationsschicht ein. Es ist also nicht verwunderlich, dass diese Ausgaben auch vom Attention-Netzwerk sehr effizient genutzt werden können, um Klassifikationsaufgaben zu lösen. Fraglich ist dabei, ob die vom Attention-Netzwerk gelernte Gewichtung bzw. Auswahl der Deskriptoren für eine erfolgreiche Klassifikation entscheidend ist, oder ob auf Grund der aussagekräftigen Deskriptoren eine beliebige Auswahl an Deskriptoren genügt, um die Klassifikationsaufgabe zu lösen. Falls die Gewichtung der Deskriptoren für das Training nur einen geringen Einfluss hat, könnte sich dies negativ auf die gelernte Selektionsfähigkeit des Attention-Netzwerks auswirken.
\\


\begin{figure}[h]
\centering
\includegraphics[scale=0.79]{attention_layer_diff}
\caption{Unterschiede der Deskriptorselektion bei Verwendung unterschiedlicher Extraktionspunkte.}
\label{attention_layer_diff}
\end{figure}
In Abbildung \ref{attention_layer_diff} ist das Auswahlverhalten der Attention-Netzwerke bei unterschiedlichen Extraktionspunkten an dem Beispiel eines Bildes der Semperoper dargestellt. Es zeigt sich, dass das Attention-Netzwerk auf Basis der Deskriptoren aus Block-4 deutlich mehr Deskriptoren außerhalb der Semperoper selektiert.
Ein Aspekt, der die Auswahl von Deskriptoren außerhalb der interessanten Bildbereiche begünstigt, ist, dass bei der Verwendung eines späteren Extraktionspunktes aufgrund der Netzwerkarchitektur eine geringere Anzahl an Deskriptoren erzeugt wird. Die negativen Auswirkungen einer geringen Auswahl an Deskriptoren wurde bereits in Abbildung \ref{small_img} auf Seite \pageref{small_img} beobachtet\footnote{Bei einer Extraktion nach Block-4 werden ca. $75 \%$ weniger Deskriptoren erzeugt als bei Block-3. Im Vergleich dazu werden nach einer Skalierung von der Originalgröße ($2.5$ MPixel) auf $0.6$ MPixel, wie in Abb. \ref{small_img} dargestellt ca. $76 \%$ weniger Deskriptoren erzeugt. Die Deskriptorauswahl auf der $0.6$ MPixel Version scheint jedoch deutlich besser zu funktionieren als mit Deskriptoren aus Block-4 in Originalgröße. Folglich scheint die Anzahl an erzeugten Deskriptoren nicht alleiniger Grund für das schlechtere Auswahlverhalten zu sein.}. Der ungewöhnliche Trainingsverlauf lässt außerdem vermuten, dass das Attention-Netzwerk in der Lage ist, die Trainingsaufgabe zu lösen, ohne ein geeignetes Selektionsverhalten zu erlernen.
\\
In Abb. \ref{mAP_layer_diff_ransac} werden die Retrievalergebnisse bei unterschiedlichem Extraktionspunkt verglichen. Wie bei vorangegangenen Experimenten werden hierbei unterschiedliche Deskriptorlängen und Distanzschwellwerte untersucht. Dabei wird mit allen untersuchten Deskriptorlängen und Distanzschwellwerten auf beiden untersuchten Datensätzen eine niedrigere mAP erreicht, wenn die verwendeten Deskriptoren aus dem vierten ResNet-Block stammen. Dabei fallen die Performanzunterschiede bei kurzen Deskriptoren besonders stark aus. Bei Verwendung des jeweils besten gefundenen Distanzschwellwerts wird bei einer Deskriptorlänge von $20$ eine im Mittel um $0.14$ niedrigere mAP auf den historischen Daten erzielt, wenn Deskriptoren aus Block-4 genutzt werden. Bei einer Deskriptorlänge von $200$ schrumpft dieser Unterschied auf $0.08$. Analog zeigt sich auf Oxford5k bei Deskriptorlänge $20$ ein Unterschied der mittleren mAP von $0.20$ und bei Länge $200$ eine Differenz von $0.05$. Die beste gefundene Konfiguration bei Verwendung von Deskriptoren aus Block-4, auf den historischen Daten nutzt eine Deskriptorlänge von $200$ und einen Distanzschwellwert von $120$. Sie erreicht eine mittlere mAP von $0.49$, was um $0.08$ niedriger ist als die beste gefundene Konfiguration mit Deskriptoren aus Block-3. Die beste gefundene Konfiguration für den Oxford5k-Datensatz mit Deskriptoren aus Block-4 nutzt ebenfalls eine Deskriptorlänge von $200$ und einen Distanzschwellwert von $1.2$ und erreicht eine mittlere mAP von $0.65$, was um $0.06$ niedriger ist als die beste gefundene Konfiguration mit Deskriptoren aus Block-3.
\begin{figure}[h]
\includegraphics[scale=0.73]{mAP_layer_diff_ransac}
\caption{Erreichte mAP bei unterschiedlicher Deskriptorlänge und Distanzschwellwert unter Verwendung unterschiedlicher Extraktionspunkte.
RANSAC wird zur geometrischen Überprüfung von Deskriptormatches eingesetzt.}
\label{mAP_layer_diff_ransac}
\end{figure}
\\
Zusammenfassend scheint die Verwendung einer der hintersten Schichten als Extraktionspunkt zu einer Verschlechterung der Retrievalperformanz zu führen. Es ist naheliegend, dass die Änderungen des Selektionsverhalten durch das Training mit Deskriptoren des späten Extraktionspunkts für einen wesentlichen Teil dieser Verschlechterung verantwortlich sind.

\section{Verfahrensvergleich}

Mit dem Wissen um die Einflüsse unterschiedlicher Verfahrensparameter soll der abschließende Teil der Auswertung dazu dienen, die Ergebnisse des DELF-Verfahrens auf den historischen Daten einzuordnen. Hierbei liegt ein besonderes Interesse darin herauszufinden, inwiefern sich das DELF-Verfahren für den Anwendungsfall auf historischen Bildern eignet und welche Stärken und Schwächen es im Vergleich zu anderen Verfahren aufweist. Für den Verfahrensvergleich stehen Ergebnisse des bereits zu Beginn kurz vorgestellten Retrievalsystems von Razavian et. al. \cite{convnet} (in Folge als ConvNet bezeichnet) zur Verfügung. Das ConvNet-Verfahren nutzt ein tiefes CNN, in diesem Fall eine VGG-16 Architektur, um mehrere lokale Deskriptoren für ein Eingangsbild zu erzeugen. Hierfür werden die Bilder zunächst regelbasiert in $30$ unterschiedlich große Teilbereiche zerschnitten. Die Teilbilder durchlaufen das VGG-Netzwerk und aus den Ausgaben der letzten faltenden Schicht wird mit Hilfe von max-pooling für jedes Teilbild ein $2048$-dimensionaler Deskriptor erzeugt. Um die Ähnlichkeit zwischen einem Anfragebild und einem Bild des Suchdatensatzes zu quantifizieren, wird für jeden Deskriptor des Anfragebildes die euklidische Distanz zum ähnlichsten Deskriptor des Suchbildes berechnet. Die summierten Distanzen dienen als Ähnlichkeitsmaß für potentielle Bildpaare.
\\
Das DELF-Verfahren nutzt für den Verfahrenvergleich die beste gefundene Konfiguration mit geometrischer Verifizierung mit einer Deskriptorlänge von $160$ und einem Distanzschwellwert von $1.0$. Da auf den historischen Daten auch ohne geometrische Verifizierung ähnlich gute Ergebnisse erzielt werden, wird außerdem eine Konfiguration mit Deskriptorlänge $200$ und Distanzschwellwert $1.0$ ohne RANSAC betrachtet. Die Deskriptoren entstammen jeweils aus den Ausgaben des dritten ResNet-Blocks. Da sich bei wiederholten Experimenten mit gleicher Konfiguration keine signifikanten Unterschiede in den Ergebnissen beobachten lassen, basieren die in Folge präsentierten Ergebnisse jeweils nur auf einzelnen Experimentalläufen.
\\
Ein Aspekt, der für den praktischen Einsatz von Retrievalsystemen bedeutsam ist, in der vorliegenden Arbeit jedoch nur eine untergeordnete Rolle spielt, ist die für eine Suchanfrage benötigte Rechenleistung. Es sei angemerkt, dass das Ziel der in der vorliegenden Arbeit erstellten Implementierung ist, einen möglichst flexiblen und nachvollziehbaren Prototypen zu erstellen. Daher ist für die praktische Anwendung des Verfahrens noch Optimierungspotential vorhanden. Trotzdem lässt sich die Größenordnung der möglichen Laufzeit mit Hilfe des Prototyps grob einschätzen. Um die Laufzeit der Verfahren zu Vergleichen, führen alle untersuchten Verfahren Retrivalexperimente auf den historischen Daten durch, wobei ihnen je $21$ Kerne eines IBM Power9 CPUs und eine NVIDIA VOLTA V100 GPU zur Verfügung stehen. Das DELF-Verfahren unter Verwendung von RANSAC benötigt ca. $54$ Sekunden um einen Datensatz von $1000$ Bildern mit einem Anfragebild zu vergleichen. Hierbei ist die geometrische Verifikation für den Großteil der benötigten Rechenzeit verantwortlich. Verwendet man DELF ohne geometrische Verifikation, dauern $1000$ Vergleiche nur ca. $18$ Sekunden. Ohne RANSAC ist die Laufzeit von DELF ähnlich wie die des ConvNet-Verfahrens, welche für $1000$ Vergleiche zwischen $14$ und $19$ Sekunden beträgt. 
\\
\begin{table}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
\rowcolor[HTML]{C0C0C0} 
Kategorie        & Frauenkirche & Hofkirche & Moritzburg & Semperoper & Sophienkirche & Stallhof & Zwinger & Gesamt \\ \hline
DELF mit RANSAC  & 0.62         & 0.74      & 0.73       & 0.33       & 0.34          & 0.43     & 0.69    & 0.56   \\ \hline
DELF ohne RANSAC & 0.51         & 0.66      & 0.86       & 0.36       & 0.37          & 0.61     & 0.65    & 0.56   \\ \hline
ConvNet          & 0.53         & 0.56      & 0.83       & 0.22       & 0.29          & 0.33     & 0.78    & 0.50   \\ \hline
\end{tabular}%
}
\caption{Erreichte mAP der unterschiedlichen Verfahren bzw. Konfigurationen auf den historischen Daten, basierend auf $42$ Anfragen; dargestellt für die einzelnen Kategorien sowie für alle Anfragen. Für die einzelnen Kategorien werden unterschiedlich viele Anfragebilder betrachtet (vgl. Tab. \ref{hist4d_data}, S. \ref{hist4d_data}).}
\label{comp_bench}
\end{table}
In Bezug auf die Retrievalergebnisse auf den historischen Daten erzielen beide Versionen des DELF-Verfahrens auf den $42$ untersuchten Anfrage insgesamt eine um rund $0.06$ höhere mAP als das ConvNet-Verfahren, wobei sich je nach Objektkategorie teilweise starke Unterschiede zeigen (vgl. Tab. \ref{comp_bench}). Hierbei gibt es einige Kategorien wie die Semperoper oder Sophienkirche die für alle getesteten Verfahren eine besonders große Herausforderung darstellen. Neben Aspekten wie Auflösung, Aufnahmequalität und Klassenverteilung innerhalb des Datensatzes (vgl. Tab. \ref{hist4d_data}) gibt es speziell historische Einflüsse, die die Retrievalaufgabe für diese Kategorien besonders schwierig gestalten. Betrachtet man die Anfragebilder, welche mit den getesteten Verfahren am schlechtesten beantwortet wurden (vgl. Abb. \ref{worst_queries}), so sieht man, dass sich die hier gezeigten Aufnahmen der Semperoper und des Stallhofs architektonisch deutlich von ihrem heutigen Erscheinungsbild unterscheiden. Innerhalb des Datensatzes sind unterschiedliche bauliche Stadien dieser Sehenswürdigkeiten enthalten, was ein genaues Matching erschwert. Auch zahlreiche Abbildungen von zerstörten Objekten wie beispielsweise der Sophienkirchen sind ihren intakten Versionen nur schwer zuordenbar.  
\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.40]{worst_queries}
\caption{Anfragebilder, die unter Verwendung der unterschiedlichen Verfahren die niedrigste AP erreichen. Von links nach rechts, sortiert nach aufsteigender maximal erreichter AP: Semperoper, Sophienkirche, Stallhof, Semperoper.}
\label{worst_queries}
\end{figure}
In Abbildung \ref{destruction_pr} ist der Verlauf einer Suchanfrage zu einer Abbildung der Sophienkirche für die unterschiedlichen Verfahren anhand von PR-Kurven dargestellt. Bei der Sophienkirche handelt es sich um die Kategorie, welche im Datensatz prozentual am häufigsten (teilweise) zerstört dargestellt ist (vgl. Tab. \ref{challenging_imgs}). Die Abbildungen der zerstörten Sophienkirchen werden zu einem überwiegenden Teil erst sehr spät innerhalb der Anfrageantworten zurückgegeben, wenn die Precision bereits sehr stark gesunken ist bzw. schon viele Bilder ohne korrekten Bildinhalt zurückgegeben wurden. Daraus lässt sich schließen, dass Aufnahmen von zerstörten Gebäuden tatsächlich schwerer intakten Motiven zugeordnet werden können als Aufnahmen der unbeschädigten Bauwerke.
\\
\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{destruction_pr}
\caption{Anfrageantworten der unterschiedlichen Verfahren auf das Anfragebild, dargestellt als PR-Kurven. Kreuze heben hervor, wann Bilder mit korrektem Bildinhalt zurückgegeben werden, bei denen das Objekt teilweise zerstört dargestellt ist.}
\label{destruction_pr}
\end{figure}
Zerstörung und architektonischer Wandel sind nicht die einzigen Aspekte, die den historischen Datensatz besonders anspruchsvoll für das Image Retrieval machen. Auch ein großer Anteil an Nacht- und Luftaufnahmen machen die Suche besonders schwierig. Bei in Dunkelheit aufgenommenen Bildern, wie in Abbildung \ref{example_night_aerial} a zu sehen, lässt sich durch den geringen Kontrast kaum erkennen, welche Teile des Bildes relevante Inhalte darstellen, was die Auswahl geeigneter Deskriptoren erschwert. Zusätzlich gehen viele Details verloren, die für die Erstellung von aussagekräftigen Deskriptoren genutzt werden könnten. Generell ist davon auszugehen, dass sich Deskriptoren aus Nachtbildern deutlich von Deskriptoren aus Aufnahmen bei Tageslicht unterscheiden. 
\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{example_night_aerial}
\caption{Beispiele für besonders herausfordernde Aufnahmen im historische Datensatz in Form von Nacht- und Luftaufnahmen.}
\label{example_night_aerial}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{night_pr}
\caption{Anfrageantworten der unterschiedlichen Verfahren auf das Anfragebild, dargestellt als PR-Kurven. Kreuze heben hervor, wann Nachtaufnahmen mit korrektem Bildinhalt zurückgegeben werden.}
\label{night_pr}
\end{figure}
In Abbildung \ref{night_pr} sind PR-Kurven zu einer Suchanfrage nach Bildern des Zwingers dargestellt. Insbesondere bei Verwendung von DELF zeigt sich, dass Nachtaufnahmen des Zwinger, dargestellt durch Kreuze, fast ausschließlich unter den zuletzt zurückgegebenen Bildern zu finden sind. Interessanterweise scheint das ConvNet-Verfahren deutlich besser in der Lage zu sein, Nachtaufnahmen mit dem Bild der Suchanfrage zu matchen. Die Nachtbilder werden relativ gleichmäßig über den Verlauf der PR-Kurve zurückgegeben, was darauf schließen lässt, dass die Belichtung hier eine deutlich geringere Rolle für den Matchingerfolg spielt. Warum dies so ist, lässt sich aktuell nur schwer feststellen, es erklärt allerdings, warum das ConvNet-Verfahren ausgerechnet für die Kategorie des Zwingers deutlich bessere Ergebnisse als das DELF-Verfahren erzielt (vgl. \ref{comp_bench}). So findet sich in der Kategorie Zwinger der größte Anteil an Nachtaufnahmen im historischen Datensatz (vgl. Tab. \ref{challenging_imgs}).
\begin{table}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\rowcolor[HTML]{C0C0C0} 
Kategorie & Frauenkirche & Hofkirche & Moritzburg & Semperoper & Sophienkirche & Stallhof & Zwinger \\ \hline
Luftaufnahme  & 16.5\%       & 18.5\%    & 0.0\%      & 22.5\%     & 7.6\%         & 0.0\%    & 3.7\%   \\ \hline
Zerstörung    & 2.9\%        & 9.7\%     & 0.0\%      & 4.5\%      & 27.3\%        & 7.9\%    & 1.1\%   \\ \hline
Nachtaufnahme & 11.7\%       & 5.6\%     & 0.0\%      & 1.1\%      & 4.5\%         & 0.0\%    & 15.0\%  \\ \hline
\end{tabular}%
}
\caption{Anteil an Objektdarstellungen im historischen Datensatz, die unterschiedliche erschwerende Besonderheiten vorweisen.}
\label{challenging_imgs}
\end{table}
Luftaufnahmen, wie in Abbildung \ref{example_night_aerial} b zu sehen, stellen aus mehreren Gründen eine Herausforderung für Retrievalverfahren dar. Zunächst zeigen sie die abgebildeten Objekte aus einer ungewöhnlichen Perspektive, sodass Objektteile in den Fokus gerückt werden, die bei einer Frontalaufnahme nicht zu sehen sind. Zugleich werden große Teile der Frontansicht durch Dächer und Türme der Gebäude verdeckt. Luftaufnahmen zeigen auch meist nicht nur das gesuchte Objekt, sondern einen großen Ausschnitt der Umgebung, der häufig viele weitere markante Gebäude und Strukturen enthält. In den Luftaufnahmen des historischen Datensatzes sind auf Grund der Dresdner Stadtstruktur häufig sogar mehrere Objekte aus anderen Kategorien zu sehen. Der jeweils gesuchte Bildinhalt nimmt bei Luftaufnahmen meist nur einen kleinen Teil des Gesamtbildes ein. Dies führt nicht nur dazu, dass weniger Details des Objektes sichtbar sind, sondern auch dazu, dass ein Großteil der Deskriptoren, die aus einem Luftbild extrahiert werden, nicht das gesuchte Objekt beschreiben. Mit Hilfe des Attention-Netzwerks kann DELF zwar strukturarme Bildbereiche wie z.B. Abschnitte der Elbe ausschließen. Das Selektionsverfahren kann allerdings nicht entscheiden, welche Gebäude für die aktuelle Suchanfrage interessant sind. Das ConvNet-Verfahren, welches mit einer regelbasierten Segmentierung des Bildinhaltes arbeitet, hat keine Möglichkeit, Bildbereiche auszuschließen. In Abbildung \ref{aerial_pr} sind PR-Kurven zu einer Suchanfrage der Frauenkirche dargestellt, bei denen zurückgegebene Luftaufnahmen hervorgehoben werden, welche die Frauenkirche enthalten.
\\
Es zeigt sich deutlich, dass das Matchen von Luftaufnahmen für alle untersuchten Verfahren sehr anspruchsvoll ist. Wie erwartet hat das ConvNet-Verfahren mit seinem segmentierenden Ansatz jedoch die größten Schwierigkeiten, Luftaufnahmen zu matchen. Das DELF-Verfahren profitiert bei der Betrachtung deutlich von der geometrischen Verifikation. Bei der Verwendung von RANSAC ist die Anzahl an verifizierten Matchings auch zwischen Bildern mit gleichem Bildinhalt typischerweise deutlich niedriger als bei einer reinen Betrachtung der initialen Matchanzahl. Daher kann auch mit einer geringen Anzahl an relevanten Deskriptoren potentiell eine hohe RANSAC-Bewertung erzielt werden. Da die relevanten Deskriptoren der betrachteten Luftbilder auf einen kleinen Bildbereich eingegrenzt sind und daher nah beieinander liegen, ist es außerdem leichter, eine geeignete Transformation zu finden, welche einen großen Teil der relevanten Deskriptoren innerhalb des RANSAC-Schwellwerts erklären kann. 
\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{aerial_pr}
\caption{Anfrageantworten der unterschiedlichen Verfahren auf das Anfragebild, dargestellt als PR-Kurven. Kreuze heben hervor, wann Luftaufnahmen mit korrektem Bildinhalt zurückgegeben werden.}
\label{aerial_pr}
\end{figure}
Betrachtet man die erzielte mAP der Verfahren in den unterschiedlichen Kategorien (vgl. Tab. \ref{comp_bench}) in Zusammenhang mit den Anteilen an besonders herausfordernden Bilder je nach Kategorie (vgl. Tab. \ref{challenging_imgs}), so zeigt sich, dass DELF besonders bei einem hohen Anteil an Luftaufnahmen (z.B. von Semperoper, Hofkirche, Frauenkirche) deutlich besser als das ConvNet-Verfahren funktioniert. In der Kategorie der Frauenkirche fällt dieser Effekt, aufgrund des hohen Anteils an Nachtaufnahmen möglicherweise geringer aus.